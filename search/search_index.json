{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLMRateLimiter","text":"<p>Client-side rate limiting for LLM API calls using Redis-backed FIFO queues.</p> <p>LLMRateLimiter helps you stay within your LLM provider's rate limits (TPM/RPM) by coordinating requests across multiple processes or servers using Redis.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>FIFO Queue-Based: Requests are processed in order, preventing thundering herd problems</li> <li>Distributed: Redis-backed for multi-process and multi-server deployments</li> <li>Flexible Limits: Supports combined TPM (OpenAI/Anthropic), split input/output TPM (GCP Vertex AI), or both</li> <li>Automatic Retry: Exponential backoff with jitter for transient Redis failures</li> <li>Graceful Degradation: Allows requests through when Redis is unavailable</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install llmratelimiter\n</code></pre> <p>Or with uv:</p> <pre><code>uv add llmratelimiter\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#simple-usage","title":"Simple Usage","text":"<pre><code>from llmratelimiter import RateLimiter\n\n# Just pass a Redis URL and your limits\nlimiter = RateLimiter(\"redis://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100)\n\nawait limiter.acquire(tokens=5000)\nresponse = await openai.chat.completions.create(...)\n</code></pre>"},{"location":"#split-mode-gcp-vertex-ai","title":"Split Mode (GCP Vertex AI)","text":"<pre><code>from llmratelimiter import RateLimiter\n\n# Separate input/output token limits\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"gemini-1.5-pro\",\n    input_tpm=4_000_000, output_tpm=128_000, rpm=360\n)\n\n# Estimate output tokens upfront\nresult = await limiter.acquire(input_tokens=5000, output_tokens=2048)\nresponse = await vertex_ai.generate(...)\n\n# Adjust after getting actual output tokens\nawait limiter.adjust(result.record_id, actual_output=response.output_tokens)\n</code></pre>"},{"location":"#with-existing-redis-client","title":"With Existing Redis Client","text":"<pre><code>from redis.asyncio import Redis\nfrom llmratelimiter import RateLimiter\n\nredis = Redis(host=\"localhost\", port=6379)\nlimiter = RateLimiter(redis=redis, model=\"gpt-4\", tpm=100_000, rpm=100)\n\nawait limiter.acquire(tokens=5000)\n</code></pre>"},{"location":"#production-setup-with-retry","title":"Production Setup with Retry","text":"<pre><code>from llmratelimiter import RateLimiter, RedisConnectionManager, RetryConfig\n\n# Use connection manager for automatic retry and pooling\nmanager = RedisConnectionManager(\n    \"redis://localhost:6379\",\n    retry_config=RetryConfig(max_retries=3, base_delay=0.1),\n)\nlimiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)\n\nawait limiter.acquire(tokens=5000)\n</code></pre>"},{"location":"#ssl-connection","title":"SSL Connection","text":"<pre><code>from llmratelimiter import RateLimiter\n\n# Use rediss:// for SSL connections\nlimiter = RateLimiter(\"rediss://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Usage Guide - Detailed examples for all modes and configurations</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for LLMRateLimiter.</p>"},{"location":"api/#main-module","title":"Main Module","text":"<p>The main module exports all public classes and functions.</p> <p>LLM Rate Limiter - Client-side rate limiting for LLM API calls.</p> <p>This library provides FIFO queue-based rate limiting to prevent hitting provider rate limits (TPM/RPM) when calling LLM APIs.</p> <p>Basic usage (recommended: specify input and output tokens separately):     &gt;&gt;&gt; from llmratelimiter import RateLimiter     &gt;&gt;&gt;     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100)     &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=2000)     &gt;&gt;&gt; response = await openai.chat.completions.create(...)</p> With existing Redis client <p>from llmratelimiter import RateLimiter from redis.asyncio import Redis</p> <p>redis = Redis(host=\"localhost\", port=6379) limiter = RateLimiter(redis=redis, model=\"gpt-4\", tpm=100_000, rpm=100) await limiter.acquire(input_tokens=3000, output_tokens=2000)</p> <p>With connection manager (includes retry with exponential backoff):     &gt;&gt;&gt; from llmratelimiter import RateLimiter, RedisConnectionManager, RetryConfig     &gt;&gt;&gt;     &gt;&gt;&gt; manager = RedisConnectionManager(     ...     \"redis://localhost:6379\",     ...     retry_config=RetryConfig(max_retries=3, base_delay=0.1),     ... )     &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)     &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=2000)</p> <p>Split mode example (GCP Vertex AI):     &gt;&gt;&gt; limiter = RateLimiter(     ...     \"redis://localhost:6379\", \"gemini-1.5-pro\",     ...     input_tpm=4_000_000, output_tpm=128_000, rpm=360     ... )     &gt;&gt;&gt; result = await limiter.acquire(input_tokens=5000, output_tokens=2048)     &gt;&gt;&gt; response = await vertex_ai.generate(...)     &gt;&gt;&gt; await limiter.adjust(result.record_id, actual_output=response.output_tokens)</p> <p>AWS Bedrock with burndown rate (output tokens count 5x toward TPM):     &gt;&gt;&gt; limiter = RateLimiter(     ...     \"redis://localhost:6379\", \"claude-sonnet\",     ...     tpm=100_000, rpm=100, burndown_rate=5.0     ... )     &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=1000)     # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens</p>"},{"location":"api/#llmratelimiter.RateLimiter","title":"RateLimiter","text":"<p>Unified rate limiter for LLM API calls.</p> <p>Supports combined TPM, split TPM, or both based on the configuration.</p> Simple URL example <p>limiter = RateLimiter(\"redis://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100) await limiter.acquire(tokens=5000)</p> <p>Split mode example (GCP Vertex AI):     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gemini-1.5-pro\",     ...                       input_tpm=4_000_000, output_tpm=128_000, rpm=360)     &gt;&gt;&gt; result = await limiter.acquire(input_tokens=5000, output_tokens=2048)     &gt;&gt;&gt; await limiter.adjust(result.record_id, actual_output=1500)</p> With existing Redis client <p>limiter = RateLimiter(redis=existing_client, model=\"gpt-4\", tpm=100_000, rpm=100)</p> <p>With connection manager (includes retry support):     &gt;&gt;&gt; manager = RedisConnectionManager(\"redis://localhost\", retry_config=RetryConfig())     &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)</p> <p>With config object (advanced):     &gt;&gt;&gt; config = RateLimitConfig(tpm=100_000, rpm=100, burst_multiplier=1.5)     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gpt-4\", config=config)</p> <p>AWS Bedrock with burndown rate (output tokens count 5x):     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"claude-sonnet\",     ...                       tpm=100_000, rpm=100, burndown_rate=5.0)     &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=1000)     # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens</p>"},{"location":"api/#llmratelimiter.RateLimiter.has_combined_limit","title":"has_combined_limit  <code>property</code>","text":"<pre><code>has_combined_limit: bool\n</code></pre> <p>Whether this limiter has a combined TPM limit.</p>"},{"location":"api/#llmratelimiter.RateLimiter.is_split_mode","title":"is_split_mode  <code>property</code>","text":"<pre><code>is_split_mode: bool\n</code></pre> <p>Whether this limiter uses split input/output TPM limits.</p>"},{"location":"api/#llmratelimiter.RateLimiter.__init__","title":"__init__","text":"<pre><code>__init__(\n    redis: RedisClient | None = None,\n    model: str | None = None,\n    config: RateLimitConfig | None = None,\n    *,\n    tpm: int = 0,\n    rpm: int = 0,\n    input_tpm: int = 0,\n    output_tpm: int = 0,\n    window_seconds: int = 60,\n    burst_multiplier: float = 1.0,\n    burndown_rate: float = 1.0,\n    password: str | None = None,\n    db: int = 0,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    redis_client: Redis\n    | RedisConnectionManager\n    | None = None,\n    model_name: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>redis</code> <code>RedisClient | None</code> <p>Redis URL string, async Redis client, or RedisConnectionManager.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Name of the model (used for Redis key namespace).</p> <code>None</code> <code>config</code> <code>RateLimitConfig | None</code> <p>Configuration for rate limits (optional if using kwargs).</p> <code>None</code> <code>tpm</code> <code>int</code> <p>Combined tokens per minute limit.</p> <code>0</code> <code>rpm</code> <code>int</code> <p>Requests per minute limit.</p> <code>0</code> <code>input_tpm</code> <code>int</code> <p>Input tokens per minute limit (split mode).</p> <code>0</code> <code>output_tpm</code> <code>int</code> <p>Output tokens per minute limit (split mode).</p> <code>0</code> <code>window_seconds</code> <code>int</code> <p>Sliding window duration in seconds.</p> <code>60</code> <code>burst_multiplier</code> <code>float</code> <p>Multiplier for burst capacity.</p> <code>1.0</code> <code>burndown_rate</code> <code>float</code> <p>Output token multiplier for combined TPM (default 1.0). AWS Bedrock Claude models use 5.0.</p> <code>1.0</code> <code>password</code> <code>str | None</code> <p>Redis password (for URL connections).</p> <code>None</code> <code>db</code> <code>int</code> <p>Redis database number (for URL connections).</p> <code>0</code> <code>max_connections</code> <code>int</code> <p>Maximum connections in pool (for URL connections).</p> <code>10</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for URL-based connections.</p> <code>None</code> <code>redis_client</code> <code>Redis | RedisConnectionManager | None</code> <p>Deprecated, use 'redis' parameter.</p> <code>None</code> <code>model_name</code> <code>str | None</code> <p>Deprecated, use 'model' parameter.</p> <code>None</code>"},{"location":"api/#llmratelimiter.RateLimiter.acquire","title":"acquire  <code>async</code>","text":"<pre><code>acquire(*, tokens: int) -&gt; AcquireResult\n</code></pre><pre><code>acquire(\n    *, input_tokens: int, output_tokens: int = 0\n) -&gt; AcquireResult\n</code></pre> <pre><code>acquire(\n    *,\n    tokens: int | None = None,\n    input_tokens: int | None = None,\n    output_tokens: int = 0,\n) -&gt; AcquireResult\n</code></pre> <p>Acquire rate limit capacity.</p> <p>For combined mode with pre-calculated tokens, use tokens parameter:     await limiter.acquire(tokens=5000)     # Burndown rate is NOT applied - value is used directly</p> <p>For separate input/output tracking, use input_tokens/output_tokens:     await limiter.acquire(input_tokens=5000, output_tokens=2048)     # Burndown rate IS applied: effective = input + (burndown_rate * output)</p> <p>With burndown rate (e.g., AWS Bedrock with burndown_rate=5.0):     await limiter.acquire(input_tokens=3000, output_tokens=1000)     # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens</p> <p>Blocks until capacity is available (FIFO ordering), then returns. On Redis failure (after retries if configured), allows the request (graceful degradation).</p> <p>Note: The burndown_rate is only applied when using input_tokens/output_tokens. When using the tokens= parameter, it is assumed the burndown calculation has already been done by the caller. Split input/output TPM limits are not affected by burndown_rate.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int | None</code> <p>Pre-calculated total tokens (burndown already applied if needed).</p> <code>None</code> <code>input_tokens</code> <code>int | None</code> <p>Number of input tokens.</p> <code>None</code> <code>output_tokens</code> <code>int</code> <p>Number of output tokens (default 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>AcquireResult</code> <p>AcquireResult with slot time, wait time, queue position, and record ID.</p>"},{"location":"api/#llmratelimiter.RateLimiter.adjust","title":"adjust  <code>async</code>","text":"<pre><code>adjust(record_id: str, actual_output: int) -&gt; None\n</code></pre> <p>Adjust the output tokens for a consumption record.</p> <p>Use this when the actual output tokens differ from the estimate. This frees up capacity if actual &lt; estimated, or uses more if actual &gt; estimated.</p> <p>Parameters:</p> Name Type Description Default <code>record_id</code> <code>str</code> <p>The record ID from the acquire() result.</p> required <code>actual_output</code> <code>int</code> <p>The actual number of output tokens.</p> required"},{"location":"api/#llmratelimiter.RateLimiter.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status() -&gt; RateLimitStatus\n</code></pre> <p>Get current rate limit status.</p> <p>Returns:</p> Type Description <code>RateLimitStatus</code> <p>RateLimitStatus with current usage and limits.</p>"},{"location":"api/#llmratelimiter.RateLimitConfig","title":"RateLimitConfig  <code>dataclass</code>","text":"<p>Unified configuration for rate limiting.</p> <p>Supports combined TPM, split TPM, or both. Set unused limits to 0 to disable.</p> Combined mode only <p>RateLimitConfig(tpm=100_000, rpm=100)</p> Split mode only <p>RateLimitConfig(input_tpm=4_000_000, output_tpm=128_000, rpm=360)</p> <p>Mixed mode (all three limits):     RateLimitConfig(tpm=100_000, input_tpm=80_000, output_tpm=20_000, rpm=100)     # Request must satisfy ALL constraints</p> Disabling limits <ul> <li>Set rpm=0 to disable request rate limiting</li> <li>Set tpm=0 to disable combined token limiting</li> <li>Set input_tpm=0 or output_tpm=0 to disable that specific limit</li> </ul> <p>Burndown rate (AWS Bedrock):     RateLimitConfig(tpm=100_000, rpm=100, burndown_rate=5.0)     # TPM consumption = input_tokens + (burndown_rate * output_tokens)</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute limit. Set to 0 to disable.</p> required <code>tpm</code> <code>int</code> <p>Combined tokens per minute limit (input + output). Set to 0 to disable.</p> <code>0</code> <code>input_tpm</code> <code>int</code> <p>Input tokens per minute limit. Set to 0 to disable.</p> <code>0</code> <code>output_tpm</code> <code>int</code> <p>Output tokens per minute limit. Set to 0 to disable.</p> <code>0</code> <code>window_seconds</code> <code>int</code> <p>Sliding window duration in seconds.</p> <code>60</code> <code>burst_multiplier</code> <code>float</code> <p>Multiplier for burst capacity above base limits.</p> <code>1.0</code> <code>burndown_rate</code> <code>float</code> <p>Output token multiplier for combined TPM (default 1.0). AWS Bedrock Claude models use 5.0.</p> <code>1.0</code>"},{"location":"api/#llmratelimiter.RateLimitConfig.has_combined_limit","title":"has_combined_limit  <code>property</code>","text":"<pre><code>has_combined_limit: bool\n</code></pre> <p>Whether this config has a combined TPM limit.</p>"},{"location":"api/#llmratelimiter.RateLimitConfig.is_split_mode","title":"is_split_mode  <code>property</code>","text":"<pre><code>is_split_mode: bool\n</code></pre> <p>Whether this config uses split input/output TPM limits.</p>"},{"location":"api/#llmratelimiter.RateLimitConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration values.</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager","title":"RedisConnectionManager","text":"<p>Manages Redis connections with pooling and retry support.</p> Example with URL <p>async with RedisConnectionManager(\"redis://localhost:6379\") as manager: ...     client = manager.client ...     await client.ping()</p> <p>Example with host/port:     &gt;&gt;&gt; manager = RedisConnectionManager(     ...     host=\"localhost\",     ...     port=6379,     ...     retry_config=RetryConfig(max_retries=5, base_delay=0.2),     ... )     &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager.client","title":"client  <code>property</code>","text":"<pre><code>client: Redis\n</code></pre> <p>Get the Redis client, creating the pool if needed.</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig\n</code></pre> <p>Get the retry configuration.</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; RedisConnectionManager\n</code></pre> <p>Enter async context manager.</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(*args: Any) -&gt; None\n</code></pre> <p>Exit async context manager, closing connections.</p>"},{"location":"api/#llmratelimiter.RedisConnectionManager.__init__","title":"__init__","text":"<pre><code>__init__(\n    url: str | None = None,\n    *,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    password: str | None = None,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    decode_responses: bool = True,\n    **redis_kwargs: Any,\n) -&gt; None\n</code></pre> <p>Initialize the connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | None</code> <p>Redis URL (e.g., \"redis://localhost:6379/0\", \"rediss://...\" for SSL).</p> <code>None</code> <code>host</code> <code>str</code> <p>Redis server hostname (used if url is not provided).</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Redis server port (used if url is not provided).</p> <code>6379</code> <code>db</code> <code>int</code> <p>Redis database number.</p> <code>0</code> <code>password</code> <code>str | None</code> <p>Redis password.</p> <code>None</code> <code>max_connections</code> <code>int</code> <p>Maximum connections in the pool.</p> <code>10</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Configuration for retry behavior. Defaults to RetryConfig().</p> <code>None</code> <code>decode_responses</code> <code>bool</code> <p>Whether to decode responses to strings.</p> <code>True</code> <code>**redis_kwargs</code> <code>Any</code> <p>Additional arguments passed to Redis client.</p> <code>{}</code>"},{"location":"api/#llmratelimiter.RedisConnectionManager.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close all connections in the pool.</p>"},{"location":"api/#llmratelimiter.RetryConfig","title":"RetryConfig  <code>dataclass</code>","text":"<p>Configuration for retry behavior with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts (0 = no retries).</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Initial delay in seconds before first retry.</p> <code>0.1</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds between retries.</p> <code>5.0</code> <code>exponential_base</code> <code>float</code> <p>Multiplier for exponential backoff (delay * base^attempt).</p> <code>2.0</code> <code>jitter</code> <code>float</code> <p>Random jitter factor (0.0 to 1.0) to prevent thundering herd.</p> <code>0.1</code> Example <p>config = RetryConfig(max_retries=3, base_delay=0.1)</p>"},{"location":"api/#llmratelimiter.RetryConfig--retry-delays-01s-02s-04s-with-jitter","title":"Retry delays: ~0.1s, ~0.2s, ~0.4s (with jitter)","text":""},{"location":"api/#llmratelimiter.RetryConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration values.</p>"},{"location":"api/#llmratelimiter.AcquireResult","title":"AcquireResult  <code>dataclass</code>","text":"<p>Result from an acquire() call.</p> <p>Attributes:</p> Name Type Description <code>slot_time</code> <code>float</code> <p>The timestamp when the request is scheduled to execute.</p> <code>wait_time</code> <code>float</code> <p>Time in seconds the caller waited (or will wait).</p> <code>queue_position</code> <code>int</code> <p>Position in the FIFO queue (0 if immediate).</p> <code>record_id</code> <code>str</code> <p>Unique ID for this consumption record (for adjust()).</p>"},{"location":"api/#llmratelimiter.RateLimitStatus","title":"RateLimitStatus  <code>dataclass</code>","text":"<p>Current status of a rate limiter.</p> <p>Unified status for both combined and split mode limiters. Unused fields are set to 0.</p> <p>Combined mode (tpm &gt; 0):     - tokens_used/tokens_limit contain combined token usage     - input_tokens_used/input_tokens_limit are 0     - output_tokens_used/output_tokens_limit are 0</p> <p>Split mode (input_tpm/output_tpm &gt; 0):     - tokens_used/tokens_limit are 0     - input_tokens_used/input_tokens_limit contain input token usage     - output_tokens_used/output_tokens_limit contain output token usage</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model name this limiter is for.</p> <code>window_seconds</code> <code>int</code> <p>The sliding window duration.</p> <code>tokens_used</code> <code>int</code> <p>Current combined tokens consumed (combined mode).</p> <code>tokens_limit</code> <code>int</code> <p>Maximum combined tokens allowed (combined mode).</p> <code>input_tokens_used</code> <code>int</code> <p>Current input tokens consumed (split mode).</p> <code>input_tokens_limit</code> <code>int</code> <p>Maximum input tokens allowed (split mode).</p> <code>output_tokens_used</code> <code>int</code> <p>Current output tokens consumed (split mode).</p> <code>output_tokens_limit</code> <code>int</code> <p>Maximum output tokens allowed (split mode).</p> <code>requests_used</code> <code>int</code> <p>Current requests in the window.</p> <code>requests_limit</code> <code>int</code> <p>Maximum requests allowed per window.</p> <code>queue_depth</code> <code>int</code> <p>Number of pending requests (slot_time &gt; now).</p>"},{"location":"api/#configuration","title":"Configuration","text":"<p>Configuration dataclasses for rate limits and retry behavior.</p> <p>Configuration dataclasses for rate limiters.</p>"},{"location":"api/#llmratelimiter.config.RateLimitConfig","title":"RateLimitConfig  <code>dataclass</code>","text":"<p>Unified configuration for rate limiting.</p> <p>Supports combined TPM, split TPM, or both. Set unused limits to 0 to disable.</p> Combined mode only <p>RateLimitConfig(tpm=100_000, rpm=100)</p> Split mode only <p>RateLimitConfig(input_tpm=4_000_000, output_tpm=128_000, rpm=360)</p> <p>Mixed mode (all three limits):     RateLimitConfig(tpm=100_000, input_tpm=80_000, output_tpm=20_000, rpm=100)     # Request must satisfy ALL constraints</p> Disabling limits <ul> <li>Set rpm=0 to disable request rate limiting</li> <li>Set tpm=0 to disable combined token limiting</li> <li>Set input_tpm=0 or output_tpm=0 to disable that specific limit</li> </ul> <p>Burndown rate (AWS Bedrock):     RateLimitConfig(tpm=100_000, rpm=100, burndown_rate=5.0)     # TPM consumption = input_tokens + (burndown_rate * output_tokens)</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute limit. Set to 0 to disable.</p> required <code>tpm</code> <code>int</code> <p>Combined tokens per minute limit (input + output). Set to 0 to disable.</p> <code>0</code> <code>input_tpm</code> <code>int</code> <p>Input tokens per minute limit. Set to 0 to disable.</p> <code>0</code> <code>output_tpm</code> <code>int</code> <p>Output tokens per minute limit. Set to 0 to disable.</p> <code>0</code> <code>window_seconds</code> <code>int</code> <p>Sliding window duration in seconds.</p> <code>60</code> <code>burst_multiplier</code> <code>float</code> <p>Multiplier for burst capacity above base limits.</p> <code>1.0</code> <code>burndown_rate</code> <code>float</code> <p>Output token multiplier for combined TPM (default 1.0). AWS Bedrock Claude models use 5.0.</p> <code>1.0</code> Source code in <code>src/llmratelimiter/config.py</code> <pre><code>@dataclass(frozen=True)\nclass RateLimitConfig:\n    \"\"\"Unified configuration for rate limiting.\n\n    Supports combined TPM, split TPM, or both. Set unused limits to 0 to disable.\n\n    Combined mode only:\n        RateLimitConfig(tpm=100_000, rpm=100)\n\n    Split mode only:\n        RateLimitConfig(input_tpm=4_000_000, output_tpm=128_000, rpm=360)\n\n    Mixed mode (all three limits):\n        RateLimitConfig(tpm=100_000, input_tpm=80_000, output_tpm=20_000, rpm=100)\n        # Request must satisfy ALL constraints\n\n    Disabling limits:\n        - Set rpm=0 to disable request rate limiting\n        - Set tpm=0 to disable combined token limiting\n        - Set input_tpm=0 or output_tpm=0 to disable that specific limit\n\n    Burndown rate (AWS Bedrock):\n        RateLimitConfig(tpm=100_000, rpm=100, burndown_rate=5.0)\n        # TPM consumption = input_tokens + (burndown_rate * output_tokens)\n\n    Args:\n        rpm: Requests per minute limit. Set to 0 to disable.\n        tpm: Combined tokens per minute limit (input + output). Set to 0 to disable.\n        input_tpm: Input tokens per minute limit. Set to 0 to disable.\n        output_tpm: Output tokens per minute limit. Set to 0 to disable.\n        window_seconds: Sliding window duration in seconds.\n        burst_multiplier: Multiplier for burst capacity above base limits.\n        burndown_rate: Output token multiplier for combined TPM (default 1.0).\n            AWS Bedrock Claude models use 5.0.\n    \"\"\"\n\n    rpm: int\n    tpm: int = 0\n    input_tpm: int = 0\n    output_tpm: int = 0\n    window_seconds: int = 60\n    burst_multiplier: float = 1.0\n    burndown_rate: float = 1.0\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration values.\"\"\"\n        if self.burndown_rate &lt; 0:\n            raise ValueError(\"burndown_rate must be &gt;= 0\")\n\n    @property\n    def is_split_mode(self) -&gt; bool:\n        \"\"\"Whether this config uses split input/output TPM limits.\"\"\"\n        return self.input_tpm &gt; 0 or self.output_tpm &gt; 0\n\n    @property\n    def has_combined_limit(self) -&gt; bool:\n        \"\"\"Whether this config has a combined TPM limit.\"\"\"\n        return self.tpm &gt; 0\n</code></pre>"},{"location":"api/#llmratelimiter.config.RateLimitConfig.has_combined_limit","title":"has_combined_limit  <code>property</code>","text":"<pre><code>has_combined_limit: bool\n</code></pre> <p>Whether this config has a combined TPM limit.</p>"},{"location":"api/#llmratelimiter.config.RateLimitConfig.is_split_mode","title":"is_split_mode  <code>property</code>","text":"<pre><code>is_split_mode: bool\n</code></pre> <p>Whether this config uses split input/output TPM limits.</p>"},{"location":"api/#llmratelimiter.config.RateLimitConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration values.</p> Source code in <code>src/llmratelimiter/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration values.\"\"\"\n    if self.burndown_rate &lt; 0:\n        raise ValueError(\"burndown_rate must be &gt;= 0\")\n</code></pre>"},{"location":"api/#llmratelimiter.config.RetryConfig","title":"RetryConfig  <code>dataclass</code>","text":"<p>Configuration for retry behavior with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts (0 = no retries).</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Initial delay in seconds before first retry.</p> <code>0.1</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds between retries.</p> <code>5.0</code> <code>exponential_base</code> <code>float</code> <p>Multiplier for exponential backoff (delay * base^attempt).</p> <code>2.0</code> <code>jitter</code> <code>float</code> <p>Random jitter factor (0.0 to 1.0) to prevent thundering herd.</p> <code>0.1</code> Example <p>config = RetryConfig(max_retries=3, base_delay=0.1)</p> Source code in <code>src/llmratelimiter/config.py</code> <pre><code>@dataclass(frozen=True)\nclass RetryConfig:\n    \"\"\"Configuration for retry behavior with exponential backoff.\n\n    Args:\n        max_retries: Maximum number of retry attempts (0 = no retries).\n        base_delay: Initial delay in seconds before first retry.\n        max_delay: Maximum delay in seconds between retries.\n        exponential_base: Multiplier for exponential backoff (delay * base^attempt).\n        jitter: Random jitter factor (0.0 to 1.0) to prevent thundering herd.\n\n    Example:\n        &gt;&gt;&gt; config = RetryConfig(max_retries=3, base_delay=0.1)\n        # Retry delays: ~0.1s, ~0.2s, ~0.4s (with jitter)\n    \"\"\"\n\n    max_retries: int = 3\n    base_delay: float = 0.1\n    max_delay: float = 5.0\n    exponential_base: float = 2.0\n    jitter: float = 0.1\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration values.\"\"\"\n        if self.max_retries &lt; 0:\n            raise ValueError(\"max_retries must be &gt;= 0\")\n        if self.base_delay &lt;= 0:\n            raise ValueError(\"base_delay must be &gt; 0\")\n        if self.max_delay &lt; self.base_delay:\n            raise ValueError(\"max_delay must be &gt;= base_delay\")\n        if self.exponential_base &lt; 1:\n            raise ValueError(\"exponential_base must be &gt;= 1\")\n        if not 0 &lt;= self.jitter &lt;= 1:\n            raise ValueError(\"jitter must be between 0 and 1\")\n</code></pre>"},{"location":"api/#llmratelimiter.config.RetryConfig--retry-delays-01s-02s-04s-with-jitter","title":"Retry delays: ~0.1s, ~0.2s, ~0.4s (with jitter)","text":""},{"location":"api/#llmratelimiter.config.RetryConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration values.</p> Source code in <code>src/llmratelimiter/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration values.\"\"\"\n    if self.max_retries &lt; 0:\n        raise ValueError(\"max_retries must be &gt;= 0\")\n    if self.base_delay &lt;= 0:\n        raise ValueError(\"base_delay must be &gt; 0\")\n    if self.max_delay &lt; self.base_delay:\n        raise ValueError(\"max_delay must be &gt;= base_delay\")\n    if self.exponential_base &lt; 1:\n        raise ValueError(\"exponential_base must be &gt;= 1\")\n    if not 0 &lt;= self.jitter &lt;= 1:\n        raise ValueError(\"jitter must be between 0 and 1\")\n</code></pre>"},{"location":"api/#connection-management","title":"Connection Management","text":"<p>Redis connection pooling and retry logic.</p> <p>Redis connection management with pooling and retry support.</p>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager","title":"RedisConnectionManager","text":"<p>Manages Redis connections with pooling and retry support.</p> Example with URL <p>async with RedisConnectionManager(\"redis://localhost:6379\") as manager: ...     client = manager.client ...     await client.ping()</p> <p>Example with host/port:     &gt;&gt;&gt; manager = RedisConnectionManager(     ...     host=\"localhost\",     ...     port=6379,     ...     retry_config=RetryConfig(max_retries=5, base_delay=0.2),     ... )     &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>class RedisConnectionManager:\n    \"\"\"Manages Redis connections with pooling and retry support.\n\n    Example with URL:\n        &gt;&gt;&gt; async with RedisConnectionManager(\"redis://localhost:6379\") as manager:\n        ...     client = manager.client\n        ...     await client.ping()\n\n    Example with host/port:\n        &gt;&gt;&gt; manager = RedisConnectionManager(\n        ...     host=\"localhost\",\n        ...     port=6379,\n        ...     retry_config=RetryConfig(max_retries=5, base_delay=0.2),\n        ... )\n        &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str | None = None,\n        *,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        password: str | None = None,\n        max_connections: int = 10,\n        retry_config: RetryConfig | None = None,\n        decode_responses: bool = True,\n        **redis_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the connection manager.\n\n        Args:\n            url: Redis URL (e.g., \"redis://localhost:6379/0\", \"rediss://...\" for SSL).\n            host: Redis server hostname (used if url is not provided).\n            port: Redis server port (used if url is not provided).\n            db: Redis database number.\n            password: Redis password.\n            max_connections: Maximum connections in the pool.\n            retry_config: Configuration for retry behavior. Defaults to RetryConfig().\n            decode_responses: Whether to decode responses to strings.\n            **redis_kwargs: Additional arguments passed to Redis client.\n        \"\"\"\n        self._url = url\n        self._host = host\n        self._port = port\n        self._db = db\n        self._password = password\n        self._max_connections = max_connections\n        self._retry_config = retry_config or RetryConfig()\n        self._decode_responses = decode_responses\n        self._redis_kwargs = redis_kwargs\n\n        self._pool: ConnectionPool | None = None\n        self._client: Redis | None = None\n\n    @property\n    def retry_config(self) -&gt; RetryConfig:\n        \"\"\"Get the retry configuration.\"\"\"\n        return self._retry_config\n\n    @property\n    def client(self) -&gt; Redis:\n        \"\"\"Get the Redis client, creating the pool if needed.\"\"\"\n        if self._client is None:\n            # Build common kwargs\n            pool_kwargs: dict[str, Any] = {\n                \"max_connections\": self._max_connections,\n                \"decode_responses\": self._decode_responses,\n                **self._redis_kwargs,\n            }\n\n            if self._url is not None:\n                # Use URL-based connection pool (use rediss:// for SSL)\n                # Override db/password if explicitly provided\n                if self._db != 0:\n                    pool_kwargs[\"db\"] = self._db\n                if self._password is not None:\n                    pool_kwargs[\"password\"] = self._password\n\n                self._pool = ConnectionPool.from_url(self._url, **pool_kwargs)\n            else:\n                # Use host/port-based connection pool\n                self._pool = ConnectionPool(\n                    host=self._host,\n                    port=self._port,\n                    db=self._db,\n                    password=self._password,\n                    **pool_kwargs,\n                )\n            self._client = Redis(connection_pool=self._pool)\n        return self._client\n\n    async def close(self) -&gt; None:\n        \"\"\"Close all connections in the pool.\"\"\"\n        if self._client is not None:\n            await self._client.aclose()\n            self._client = None\n        if self._pool is not None:\n            await self._pool.disconnect()\n            self._pool = None\n\n    async def __aenter__(self) -&gt; \"RedisConnectionManager\":\n        \"\"\"Enter async context manager.\"\"\"\n        return self\n\n    async def __aexit__(self, *args: Any) -&gt; None:\n        \"\"\"Exit async context manager, closing connections.\"\"\"\n        await self.close()\n</code></pre>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.client","title":"client  <code>property</code>","text":"<pre><code>client: Redis\n</code></pre> <p>Get the Redis client, creating the pool if needed.</p>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.retry_config","title":"retry_config  <code>property</code>","text":"<pre><code>retry_config: RetryConfig\n</code></pre> <p>Get the retry configuration.</p>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; RedisConnectionManager\n</code></pre> <p>Enter async context manager.</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>async def __aenter__(self) -&gt; \"RedisConnectionManager\":\n    \"\"\"Enter async context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(*args: Any) -&gt; None\n</code></pre> <p>Exit async context manager, closing connections.</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>async def __aexit__(self, *args: Any) -&gt; None:\n    \"\"\"Exit async context manager, closing connections.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.__init__","title":"__init__","text":"<pre><code>__init__(\n    url: str | None = None,\n    *,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    password: str | None = None,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    decode_responses: bool = True,\n    **redis_kwargs: Any,\n) -&gt; None\n</code></pre> <p>Initialize the connection manager.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | None</code> <p>Redis URL (e.g., \"redis://localhost:6379/0\", \"rediss://...\" for SSL).</p> <code>None</code> <code>host</code> <code>str</code> <p>Redis server hostname (used if url is not provided).</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>Redis server port (used if url is not provided).</p> <code>6379</code> <code>db</code> <code>int</code> <p>Redis database number.</p> <code>0</code> <code>password</code> <code>str | None</code> <p>Redis password.</p> <code>None</code> <code>max_connections</code> <code>int</code> <p>Maximum connections in the pool.</p> <code>10</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Configuration for retry behavior. Defaults to RetryConfig().</p> <code>None</code> <code>decode_responses</code> <code>bool</code> <p>Whether to decode responses to strings.</p> <code>True</code> <code>**redis_kwargs</code> <code>Any</code> <p>Additional arguments passed to Redis client.</p> <code>{}</code> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>def __init__(\n    self,\n    url: str | None = None,\n    *,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    password: str | None = None,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    decode_responses: bool = True,\n    **redis_kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the connection manager.\n\n    Args:\n        url: Redis URL (e.g., \"redis://localhost:6379/0\", \"rediss://...\" for SSL).\n        host: Redis server hostname (used if url is not provided).\n        port: Redis server port (used if url is not provided).\n        db: Redis database number.\n        password: Redis password.\n        max_connections: Maximum connections in the pool.\n        retry_config: Configuration for retry behavior. Defaults to RetryConfig().\n        decode_responses: Whether to decode responses to strings.\n        **redis_kwargs: Additional arguments passed to Redis client.\n    \"\"\"\n    self._url = url\n    self._host = host\n    self._port = port\n    self._db = db\n    self._password = password\n    self._max_connections = max_connections\n    self._retry_config = retry_config or RetryConfig()\n    self._decode_responses = decode_responses\n    self._redis_kwargs = redis_kwargs\n\n    self._pool: ConnectionPool | None = None\n    self._client: Redis | None = None\n</code></pre>"},{"location":"api/#llmratelimiter.connection.RedisConnectionManager.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close all connections in the pool.</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close all connections in the pool.\"\"\"\n    if self._client is not None:\n        await self._client.aclose()\n        self._client = None\n    if self._pool is not None:\n        await self._pool.disconnect()\n        self._pool = None\n</code></pre>"},{"location":"api/#llmratelimiter.connection.calculate_delay","title":"calculate_delay","text":"<pre><code>calculate_delay(attempt: int, config: RetryConfig) -&gt; float\n</code></pre> <p>Calculate delay for a retry attempt with exponential backoff and jitter.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>The retry attempt number (0-indexed).</p> required <code>config</code> <code>RetryConfig</code> <p>Retry configuration.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds before the next retry.</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>def calculate_delay(attempt: int, config: RetryConfig) -&gt; float:\n    \"\"\"Calculate delay for a retry attempt with exponential backoff and jitter.\n\n    Args:\n        attempt: The retry attempt number (0-indexed).\n        config: Retry configuration.\n\n    Returns:\n        Delay in seconds before the next retry.\n    \"\"\"\n    # Exponential backoff: base_delay * (exponential_base ** attempt)\n    delay = config.base_delay * (config.exponential_base**attempt)\n\n    # Cap at max_delay\n    delay = min(delay, config.max_delay)\n\n    # Add jitter: \u00b1jitter% randomization\n    if config.jitter &gt; 0:\n        jitter_range = delay * config.jitter\n        delay += random.uniform(-jitter_range, jitter_range)\n\n    return max(0, delay)  # Never negative\n</code></pre>"},{"location":"api/#llmratelimiter.connection.retry_with_backoff","title":"retry_with_backoff  <code>async</code>","text":"<pre><code>retry_with_backoff(\n    operation: Callable[[], Awaitable[T]],\n    config: RetryConfig,\n    operation_name: str = \"operation\",\n) -&gt; T\n</code></pre> <p>Execute an async operation with exponential backoff retry.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>Callable[[], Awaitable[T]]</code> <p>Async callable to execute.</p> required <code>config</code> <code>RetryConfig</code> <p>Retry configuration.</p> required <code>operation_name</code> <code>str</code> <p>Name for logging purposes.</p> <code>'operation'</code> <p>Returns:</p> Type Description <code>T</code> <p>Result of the operation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>The last exception if all retries are exhausted.</p> Source code in <code>src/llmratelimiter/connection.py</code> <pre><code>async def retry_with_backoff(\n    operation: Callable[[], Awaitable[T]],\n    config: RetryConfig,\n    operation_name: str = \"operation\",\n) -&gt; T:\n    \"\"\"Execute an async operation with exponential backoff retry.\n\n    Args:\n        operation: Async callable to execute.\n        config: Retry configuration.\n        operation_name: Name for logging purposes.\n\n    Returns:\n        Result of the operation.\n\n    Raises:\n        Exception: The last exception if all retries are exhausted.\n    \"\"\"\n    last_exception: Exception | None = None\n\n    for attempt in range(config.max_retries + 1):  # +1 for initial attempt\n        try:\n            return await operation()\n        except NON_RETRYABLE_ERRORS:\n            # Don't retry these - re-raise immediately\n            raise\n        except RETRYABLE_ERRORS as e:\n            last_exception = e\n\n            if attempt &lt; config.max_retries:\n                delay = calculate_delay(attempt, config)\n                logger.warning(\n                    \"%s failed (attempt %d/%d), retrying in %.2fs: %s\",\n                    operation_name,\n                    attempt + 1,\n                    config.max_retries + 1,\n                    delay,\n                    e,\n                )\n                await asyncio.sleep(delay)\n            else:\n                logger.warning(\n                    \"%s failed after %d attempts: %s\",\n                    operation_name,\n                    config.max_retries + 1,\n                    e,\n                )\n        except Exception:\n            # Unknown error - log and re-raise\n            logger.exception(\"Unexpected error in %s\", operation_name)\n            raise\n\n    # All retries exhausted\n    if last_exception is not None:\n        raise last_exception\n\n    # Should never reach here, but satisfy type checker\n    raise RuntimeError(\"Retry logic error\")\n</code></pre>"},{"location":"api/#rate-limiter","title":"Rate Limiter","text":"<p>The main rate limiter implementation.</p> <p>Unified rate limiter implementation.</p>"},{"location":"api/#llmratelimiter.limiter.RateLimiter","title":"RateLimiter","text":"<p>Unified rate limiter for LLM API calls.</p> <p>Supports combined TPM, split TPM, or both based on the configuration.</p> Simple URL example <p>limiter = RateLimiter(\"redis://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100) await limiter.acquire(tokens=5000)</p> <p>Split mode example (GCP Vertex AI):     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gemini-1.5-pro\",     ...                       input_tpm=4_000_000, output_tpm=128_000, rpm=360)     &gt;&gt;&gt; result = await limiter.acquire(input_tokens=5000, output_tokens=2048)     &gt;&gt;&gt; await limiter.adjust(result.record_id, actual_output=1500)</p> With existing Redis client <p>limiter = RateLimiter(redis=existing_client, model=\"gpt-4\", tpm=100_000, rpm=100)</p> <p>With connection manager (includes retry support):     &gt;&gt;&gt; manager = RedisConnectionManager(\"redis://localhost\", retry_config=RetryConfig())     &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)</p> <p>With config object (advanced):     &gt;&gt;&gt; config = RateLimitConfig(tpm=100_000, rpm=100, burst_multiplier=1.5)     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gpt-4\", config=config)</p> <p>AWS Bedrock with burndown rate (output tokens count 5x):     &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"claude-sonnet\",     ...                       tpm=100_000, rpm=100, burndown_rate=5.0)     &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=1000)     # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens</p> Source code in <code>src/llmratelimiter/limiter.py</code> <pre><code>class RateLimiter:\n    \"\"\"Unified rate limiter for LLM API calls.\n\n    Supports combined TPM, split TPM, or both based on the configuration.\n\n    Simple URL example:\n        &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100)\n        &gt;&gt;&gt; await limiter.acquire(tokens=5000)\n\n    Split mode example (GCP Vertex AI):\n        &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gemini-1.5-pro\",\n        ...                       input_tpm=4_000_000, output_tpm=128_000, rpm=360)\n        &gt;&gt;&gt; result = await limiter.acquire(input_tokens=5000, output_tokens=2048)\n        &gt;&gt;&gt; await limiter.adjust(result.record_id, actual_output=1500)\n\n    With existing Redis client:\n        &gt;&gt;&gt; limiter = RateLimiter(redis=existing_client, model=\"gpt-4\", tpm=100_000, rpm=100)\n\n    With connection manager (includes retry support):\n        &gt;&gt;&gt; manager = RedisConnectionManager(\"redis://localhost\", retry_config=RetryConfig())\n        &gt;&gt;&gt; limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)\n\n    With config object (advanced):\n        &gt;&gt;&gt; config = RateLimitConfig(tpm=100_000, rpm=100, burst_multiplier=1.5)\n        &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"gpt-4\", config=config)\n\n    AWS Bedrock with burndown rate (output tokens count 5x):\n        &gt;&gt;&gt; limiter = RateLimiter(\"redis://localhost\", \"claude-sonnet\",\n        ...                       tpm=100_000, rpm=100, burndown_rate=5.0)\n        &gt;&gt;&gt; await limiter.acquire(input_tokens=3000, output_tokens=1000)\n        # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens\n    \"\"\"\n\n    def __init__(\n        self,\n        redis: RedisClient | None = None,\n        model: str | None = None,\n        config: RateLimitConfig | None = None,\n        *,\n        # Rate limit kwargs (alternative to config)\n        tpm: int = 0,\n        rpm: int = 0,\n        input_tpm: int = 0,\n        output_tpm: int = 0,\n        window_seconds: int = 60,\n        burst_multiplier: float = 1.0,\n        burndown_rate: float = 1.0,\n        # Redis connection kwargs (for URL connections)\n        password: str | None = None,\n        db: int = 0,\n        max_connections: int = 10,\n        retry_config: RetryConfig | None = None,\n        # Legacy positional support\n        redis_client: Redis | RedisConnectionManager | None = None,\n        model_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the rate limiter.\n\n        Args:\n            redis: Redis URL string, async Redis client, or RedisConnectionManager.\n            model: Name of the model (used for Redis key namespace).\n            config: Configuration for rate limits (optional if using kwargs).\n            tpm: Combined tokens per minute limit.\n            rpm: Requests per minute limit.\n            input_tpm: Input tokens per minute limit (split mode).\n            output_tpm: Output tokens per minute limit (split mode).\n            window_seconds: Sliding window duration in seconds.\n            burst_multiplier: Multiplier for burst capacity.\n            burndown_rate: Output token multiplier for combined TPM (default 1.0).\n                AWS Bedrock Claude models use 5.0.\n            password: Redis password (for URL connections).\n            db: Redis database number (for URL connections).\n            max_connections: Maximum connections in pool (for URL connections).\n            retry_config: Retry configuration for URL-based connections.\n            redis_client: Deprecated, use 'redis' parameter.\n            model_name: Deprecated, use 'model' parameter.\n        \"\"\"\n        # Handle legacy parameter names for backward compatibility\n        if redis_client is not None and redis is None:\n            redis = redis_client\n        if model_name is not None and model is None:\n            model = model_name\n\n        if redis is None:\n            raise ValueError(\"redis parameter is required (URL string, Redis client, or RedisConnectionManager)\")\n        if model is None:\n            raise ValueError(\"model parameter is required\")\n\n        # Handle different redis parameter types\n        if isinstance(redis, str):\n            # URL string - create a connection manager\n            self._manager: RedisConnectionManager | None = RedisConnectionManager(\n                url=redis,\n                password=password,\n                db=db,\n                max_connections=max_connections,\n                retry_config=retry_config,\n            )\n            self.redis = self._manager.client\n            self._retry_config: RetryConfig | None = self._manager.retry_config\n        elif isinstance(redis, RedisConnectionManager):\n            self._manager = redis\n            self.redis = redis.client\n            self._retry_config = redis.retry_config\n        else:\n            # Raw Redis client\n            self._manager = None\n            self.redis = redis\n            self._retry_config = retry_config\n\n        self.model_name = model\n\n        # Build config from kwargs if not provided\n        if config is None:\n            config = RateLimitConfig(\n                tpm=tpm,\n                rpm=rpm,\n                input_tpm=input_tpm,\n                output_tpm=output_tpm,\n                window_seconds=window_seconds,\n                burst_multiplier=burst_multiplier,\n                burndown_rate=burndown_rate,\n            )\n\n        self.window_seconds = config.window_seconds\n        self.burst_multiplier = config.burst_multiplier\n        self._burndown_rate = config.burndown_rate\n        self._config = config\n\n        # Calculate effective limits with burst multiplier\n        self.rpm_limit = int(config.rpm * config.burst_multiplier) if config.rpm &gt; 0 else 0\n        self.tpm_limit = int(config.tpm * config.burst_multiplier) if config.tpm &gt; 0 else 0\n        self.input_tpm_limit = int(config.input_tpm * config.burst_multiplier) if config.input_tpm &gt; 0 else 0\n        self.output_tpm_limit = int(config.output_tpm * config.burst_multiplier) if config.output_tpm &gt; 0 else 0\n\n        # Redis key for consumption records\n        self.consumption_key = f\"rate_limit:{model}:consumption\"\n\n        # Lua scripts\n        self._acquire_script = ACQUIRE_SCRIPT\n        self._adjust_script = ADJUST_SCRIPT\n        self._status_script = STATUS_SCRIPT\n\n        # For testing - can be set to False to skip actual waiting\n        self._should_wait = True\n\n    @property\n    def is_split_mode(self) -&gt; bool:\n        \"\"\"Whether this limiter uses split input/output TPM limits.\"\"\"\n        return self._config.is_split_mode\n\n    @property\n    def has_combined_limit(self) -&gt; bool:\n        \"\"\"Whether this limiter has a combined TPM limit.\"\"\"\n        return self._config.has_combined_limit\n\n    @overload\n    async def acquire(self, *, tokens: int) -&gt; AcquireResult:\n        \"\"\"Acquire for combined mode - tokens counted as input.\"\"\"\n        ...\n\n    @overload\n    async def acquire(self, *, input_tokens: int, output_tokens: int = 0) -&gt; AcquireResult:\n        \"\"\"Acquire for split/mixed mode.\"\"\"\n        ...\n\n    async def acquire(\n        self,\n        *,\n        tokens: int | None = None,\n        input_tokens: int | None = None,\n        output_tokens: int = 0,\n    ) -&gt; AcquireResult:\n        \"\"\"Acquire rate limit capacity.\n\n        For combined mode with pre-calculated tokens, use tokens parameter:\n            await limiter.acquire(tokens=5000)\n            # Burndown rate is NOT applied - value is used directly\n\n        For separate input/output tracking, use input_tokens/output_tokens:\n            await limiter.acquire(input_tokens=5000, output_tokens=2048)\n            # Burndown rate IS applied: effective = input + (burndown_rate * output)\n\n        With burndown rate (e.g., AWS Bedrock with burndown_rate=5.0):\n            await limiter.acquire(input_tokens=3000, output_tokens=1000)\n            # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens\n\n        Blocks until capacity is available (FIFO ordering), then returns.\n        On Redis failure (after retries if configured), allows the request\n        (graceful degradation).\n\n        Note: The burndown_rate is only applied when using input_tokens/output_tokens.\n        When using the tokens= parameter, it is assumed the burndown calculation\n        has already been done by the caller. Split input/output TPM limits\n        are not affected by burndown_rate.\n\n        Args:\n            tokens: Pre-calculated total tokens (burndown already applied if needed).\n            input_tokens: Number of input tokens.\n            output_tokens: Number of output tokens (default 0).\n\n        Returns:\n            AcquireResult with slot time, wait time, queue position, and record ID.\n        \"\"\"\n        # Resolve input tokens and determine if burndown rate should be applied\n        if tokens is not None:\n            if input_tokens is not None:\n                raise ValueError(\"Cannot specify both tokens and input_tokens\")\n            # When tokens= is used, assume burndown is already applied\n            # Use the value directly as effective_combined_tokens\n            input_tokens = tokens\n            effective_combined_tokens = float(tokens)\n        else:\n            if input_tokens is None:\n                raise ValueError(\"Must specify either tokens or input_tokens\")\n            # When input_tokens/output_tokens are used, apply burndown rate\n            effective_combined_tokens = input_tokens + (self._burndown_rate * output_tokens)\n\n        return await self._execute_acquire(\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            effective_combined_tokens=effective_combined_tokens,\n        )\n\n    async def adjust(self, record_id: str, actual_output: int) -&gt; None:\n        \"\"\"Adjust the output tokens for a consumption record.\n\n        Use this when the actual output tokens differ from the estimate.\n        This frees up capacity if actual &lt; estimated, or uses more if actual &gt; estimated.\n\n        Args:\n            record_id: The record ID from the acquire() result.\n            actual_output: The actual number of output tokens.\n        \"\"\"\n\n        async def do_adjust() -&gt; None:\n            result = await self.redis.eval(  # type: ignore[misc]\n                self._adjust_script,\n                1,\n                self.consumption_key,\n                record_id,\n                actual_output,\n            )\n            if result[0] == 0:\n                logger.warning(\"Record not found for adjustment: %s\", record_id)\n\n        try:\n            if self._retry_config is not None:\n                await retry_with_backoff(do_adjust, self._retry_config, \"adjust\")\n            else:\n                await do_adjust()\n        except RETRYABLE_ERRORS as e:\n            logger.warning(\"Failed to adjust record %s: %s\", record_id, e)\n        except Exception as e:\n            logger.warning(\"Failed to adjust record %s: %s\", record_id, e)\n\n    async def get_status(self) -&gt; RateLimitStatus:\n        \"\"\"Get current rate limit status.\n\n        Returns:\n            RateLimitStatus with current usage and limits.\n        \"\"\"\n        current_time = time.time()\n\n        async def do_get_status() -&gt; tuple[int, int, int, int]:\n            result = await self.redis.eval(  # type: ignore[misc]\n                self._status_script,\n                1,\n                self.consumption_key,\n                current_time,\n                self.window_seconds,\n            )\n            return (\n                int(result[0]),\n                int(result[1]),\n                int(result[2]),\n                int(result[3]),\n            )\n\n        try:\n            if self._retry_config is not None:\n                total_input, total_output, total_requests, queue_depth = await retry_with_backoff(\n                    do_get_status, self._retry_config, \"get_status\"\n                )\n            else:\n                total_input, total_output, total_requests, queue_depth = await do_get_status()\n        except Exception as e:\n            logger.warning(\"Redis error getting status: %s\", e)\n            total_input = 0\n            total_output = 0\n            total_requests = 0\n            queue_depth = 0\n\n        return RateLimitStatus(\n            model=self.model_name,\n            window_seconds=self.window_seconds,\n            tokens_used=total_input + total_output,\n            tokens_limit=self.tpm_limit,\n            input_tokens_used=total_input,\n            input_tokens_limit=self.input_tpm_limit,\n            output_tokens_used=total_output,\n            output_tokens_limit=self.output_tpm_limit,\n            requests_used=total_requests,\n            requests_limit=self.rpm_limit,\n            queue_depth=queue_depth,\n        )\n\n    async def _execute_acquire(\n        self,\n        input_tokens: int,\n        output_tokens: int,\n        effective_combined_tokens: float,\n    ) -&gt; AcquireResult:\n        \"\"\"Execute the acquire operation with the Lua script.\n\n        Args:\n            input_tokens: Number of input tokens.\n            output_tokens: Number of output tokens.\n            effective_combined_tokens: Pre-calculated combined tokens (with burndown rate if applicable).\n\n        Returns:\n            AcquireResult with slot time, wait time, queue position, and record ID.\n        \"\"\"\n        current_time = time.time()\n        record_id = str(uuid.uuid4())\n\n        async def do_acquire() -&gt; tuple[float, int, str, float]:\n            result = await self.redis.eval(  # type: ignore[misc]\n                self._acquire_script,\n                1,  # number of keys\n                self.consumption_key,\n                input_tokens,\n                output_tokens,\n                self.tpm_limit,  # combined limit (0 = disabled)\n                self.input_tpm_limit,  # input limit (0 = disabled)\n                self.output_tpm_limit,  # output limit (0 = disabled)\n                self.rpm_limit,  # request limit (0 = disabled)\n                self.window_seconds,\n                current_time,\n                record_id,\n                effective_combined_tokens,  # pre-calculated with burndown rate\n            )\n            return (\n                float(result[0]),\n                int(result[1]),\n                str(result[2]),\n                float(result[3]),\n            )\n\n        try:\n            if self._retry_config is not None:\n                slot_time, queue_position, returned_record_id, wait_time = await retry_with_backoff(\n                    do_acquire, self._retry_config, \"acquire\"\n                )\n            else:\n                slot_time, queue_position, returned_record_id, wait_time = await do_acquire()\n\n            # Wait if needed\n            if self._should_wait and wait_time &gt; 0:\n                logger.debug(\n                    \"Rate limited: waiting %.2fs (queue position %d)\",\n                    wait_time,\n                    queue_position,\n                )\n                await asyncio.sleep(wait_time)\n\n            return AcquireResult(\n                slot_time=slot_time,\n                wait_time=wait_time,\n                queue_position=queue_position,\n                record_id=returned_record_id,\n            )\n\n        except Exception as e:\n            # Graceful degradation - allow request on Redis failure\n            logger.warning(\"Redis error, allowing request: %s\", e)\n            return AcquireResult(\n                slot_time=current_time,\n                wait_time=0.0,\n                queue_position=0,\n                record_id=record_id,\n            )\n</code></pre>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.has_combined_limit","title":"has_combined_limit  <code>property</code>","text":"<pre><code>has_combined_limit: bool\n</code></pre> <p>Whether this limiter has a combined TPM limit.</p>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.is_split_mode","title":"is_split_mode  <code>property</code>","text":"<pre><code>is_split_mode: bool\n</code></pre> <p>Whether this limiter uses split input/output TPM limits.</p>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.__init__","title":"__init__","text":"<pre><code>__init__(\n    redis: RedisClient | None = None,\n    model: str | None = None,\n    config: RateLimitConfig | None = None,\n    *,\n    tpm: int = 0,\n    rpm: int = 0,\n    input_tpm: int = 0,\n    output_tpm: int = 0,\n    window_seconds: int = 60,\n    burst_multiplier: float = 1.0,\n    burndown_rate: float = 1.0,\n    password: str | None = None,\n    db: int = 0,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    redis_client: Redis\n    | RedisConnectionManager\n    | None = None,\n    model_name: str | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>redis</code> <code>RedisClient | None</code> <p>Redis URL string, async Redis client, or RedisConnectionManager.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>Name of the model (used for Redis key namespace).</p> <code>None</code> <code>config</code> <code>RateLimitConfig | None</code> <p>Configuration for rate limits (optional if using kwargs).</p> <code>None</code> <code>tpm</code> <code>int</code> <p>Combined tokens per minute limit.</p> <code>0</code> <code>rpm</code> <code>int</code> <p>Requests per minute limit.</p> <code>0</code> <code>input_tpm</code> <code>int</code> <p>Input tokens per minute limit (split mode).</p> <code>0</code> <code>output_tpm</code> <code>int</code> <p>Output tokens per minute limit (split mode).</p> <code>0</code> <code>window_seconds</code> <code>int</code> <p>Sliding window duration in seconds.</p> <code>60</code> <code>burst_multiplier</code> <code>float</code> <p>Multiplier for burst capacity.</p> <code>1.0</code> <code>burndown_rate</code> <code>float</code> <p>Output token multiplier for combined TPM (default 1.0). AWS Bedrock Claude models use 5.0.</p> <code>1.0</code> <code>password</code> <code>str | None</code> <p>Redis password (for URL connections).</p> <code>None</code> <code>db</code> <code>int</code> <p>Redis database number (for URL connections).</p> <code>0</code> <code>max_connections</code> <code>int</code> <p>Maximum connections in pool (for URL connections).</p> <code>10</code> <code>retry_config</code> <code>RetryConfig | None</code> <p>Retry configuration for URL-based connections.</p> <code>None</code> <code>redis_client</code> <code>Redis | RedisConnectionManager | None</code> <p>Deprecated, use 'redis' parameter.</p> <code>None</code> <code>model_name</code> <code>str | None</code> <p>Deprecated, use 'model' parameter.</p> <code>None</code> Source code in <code>src/llmratelimiter/limiter.py</code> <pre><code>def __init__(\n    self,\n    redis: RedisClient | None = None,\n    model: str | None = None,\n    config: RateLimitConfig | None = None,\n    *,\n    # Rate limit kwargs (alternative to config)\n    tpm: int = 0,\n    rpm: int = 0,\n    input_tpm: int = 0,\n    output_tpm: int = 0,\n    window_seconds: int = 60,\n    burst_multiplier: float = 1.0,\n    burndown_rate: float = 1.0,\n    # Redis connection kwargs (for URL connections)\n    password: str | None = None,\n    db: int = 0,\n    max_connections: int = 10,\n    retry_config: RetryConfig | None = None,\n    # Legacy positional support\n    redis_client: Redis | RedisConnectionManager | None = None,\n    model_name: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the rate limiter.\n\n    Args:\n        redis: Redis URL string, async Redis client, or RedisConnectionManager.\n        model: Name of the model (used for Redis key namespace).\n        config: Configuration for rate limits (optional if using kwargs).\n        tpm: Combined tokens per minute limit.\n        rpm: Requests per minute limit.\n        input_tpm: Input tokens per minute limit (split mode).\n        output_tpm: Output tokens per minute limit (split mode).\n        window_seconds: Sliding window duration in seconds.\n        burst_multiplier: Multiplier for burst capacity.\n        burndown_rate: Output token multiplier for combined TPM (default 1.0).\n            AWS Bedrock Claude models use 5.0.\n        password: Redis password (for URL connections).\n        db: Redis database number (for URL connections).\n        max_connections: Maximum connections in pool (for URL connections).\n        retry_config: Retry configuration for URL-based connections.\n        redis_client: Deprecated, use 'redis' parameter.\n        model_name: Deprecated, use 'model' parameter.\n    \"\"\"\n    # Handle legacy parameter names for backward compatibility\n    if redis_client is not None and redis is None:\n        redis = redis_client\n    if model_name is not None and model is None:\n        model = model_name\n\n    if redis is None:\n        raise ValueError(\"redis parameter is required (URL string, Redis client, or RedisConnectionManager)\")\n    if model is None:\n        raise ValueError(\"model parameter is required\")\n\n    # Handle different redis parameter types\n    if isinstance(redis, str):\n        # URL string - create a connection manager\n        self._manager: RedisConnectionManager | None = RedisConnectionManager(\n            url=redis,\n            password=password,\n            db=db,\n            max_connections=max_connections,\n            retry_config=retry_config,\n        )\n        self.redis = self._manager.client\n        self._retry_config: RetryConfig | None = self._manager.retry_config\n    elif isinstance(redis, RedisConnectionManager):\n        self._manager = redis\n        self.redis = redis.client\n        self._retry_config = redis.retry_config\n    else:\n        # Raw Redis client\n        self._manager = None\n        self.redis = redis\n        self._retry_config = retry_config\n\n    self.model_name = model\n\n    # Build config from kwargs if not provided\n    if config is None:\n        config = RateLimitConfig(\n            tpm=tpm,\n            rpm=rpm,\n            input_tpm=input_tpm,\n            output_tpm=output_tpm,\n            window_seconds=window_seconds,\n            burst_multiplier=burst_multiplier,\n            burndown_rate=burndown_rate,\n        )\n\n    self.window_seconds = config.window_seconds\n    self.burst_multiplier = config.burst_multiplier\n    self._burndown_rate = config.burndown_rate\n    self._config = config\n\n    # Calculate effective limits with burst multiplier\n    self.rpm_limit = int(config.rpm * config.burst_multiplier) if config.rpm &gt; 0 else 0\n    self.tpm_limit = int(config.tpm * config.burst_multiplier) if config.tpm &gt; 0 else 0\n    self.input_tpm_limit = int(config.input_tpm * config.burst_multiplier) if config.input_tpm &gt; 0 else 0\n    self.output_tpm_limit = int(config.output_tpm * config.burst_multiplier) if config.output_tpm &gt; 0 else 0\n\n    # Redis key for consumption records\n    self.consumption_key = f\"rate_limit:{model}:consumption\"\n\n    # Lua scripts\n    self._acquire_script = ACQUIRE_SCRIPT\n    self._adjust_script = ADJUST_SCRIPT\n    self._status_script = STATUS_SCRIPT\n\n    # For testing - can be set to False to skip actual waiting\n    self._should_wait = True\n</code></pre>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.acquire","title":"acquire  <code>async</code>","text":"<pre><code>acquire(*, tokens: int) -&gt; AcquireResult\n</code></pre><pre><code>acquire(\n    *, input_tokens: int, output_tokens: int = 0\n) -&gt; AcquireResult\n</code></pre> <pre><code>acquire(\n    *,\n    tokens: int | None = None,\n    input_tokens: int | None = None,\n    output_tokens: int = 0,\n) -&gt; AcquireResult\n</code></pre> <p>Acquire rate limit capacity.</p> <p>For combined mode with pre-calculated tokens, use tokens parameter:     await limiter.acquire(tokens=5000)     # Burndown rate is NOT applied - value is used directly</p> <p>For separate input/output tracking, use input_tokens/output_tokens:     await limiter.acquire(input_tokens=5000, output_tokens=2048)     # Burndown rate IS applied: effective = input + (burndown_rate * output)</p> <p>With burndown rate (e.g., AWS Bedrock with burndown_rate=5.0):     await limiter.acquire(input_tokens=3000, output_tokens=1000)     # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens</p> <p>Blocks until capacity is available (FIFO ordering), then returns. On Redis failure (after retries if configured), allows the request (graceful degradation).</p> <p>Note: The burndown_rate is only applied when using input_tokens/output_tokens. When using the tokens= parameter, it is assumed the burndown calculation has already been done by the caller. Split input/output TPM limits are not affected by burndown_rate.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int | None</code> <p>Pre-calculated total tokens (burndown already applied if needed).</p> <code>None</code> <code>input_tokens</code> <code>int | None</code> <p>Number of input tokens.</p> <code>None</code> <code>output_tokens</code> <code>int</code> <p>Number of output tokens (default 0).</p> <code>0</code> <p>Returns:</p> Type Description <code>AcquireResult</code> <p>AcquireResult with slot time, wait time, queue position, and record ID.</p> Source code in <code>src/llmratelimiter/limiter.py</code> <pre><code>async def acquire(\n    self,\n    *,\n    tokens: int | None = None,\n    input_tokens: int | None = None,\n    output_tokens: int = 0,\n) -&gt; AcquireResult:\n    \"\"\"Acquire rate limit capacity.\n\n    For combined mode with pre-calculated tokens, use tokens parameter:\n        await limiter.acquire(tokens=5000)\n        # Burndown rate is NOT applied - value is used directly\n\n    For separate input/output tracking, use input_tokens/output_tokens:\n        await limiter.acquire(input_tokens=5000, output_tokens=2048)\n        # Burndown rate IS applied: effective = input + (burndown_rate * output)\n\n    With burndown rate (e.g., AWS Bedrock with burndown_rate=5.0):\n        await limiter.acquire(input_tokens=3000, output_tokens=1000)\n        # TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens\n\n    Blocks until capacity is available (FIFO ordering), then returns.\n    On Redis failure (after retries if configured), allows the request\n    (graceful degradation).\n\n    Note: The burndown_rate is only applied when using input_tokens/output_tokens.\n    When using the tokens= parameter, it is assumed the burndown calculation\n    has already been done by the caller. Split input/output TPM limits\n    are not affected by burndown_rate.\n\n    Args:\n        tokens: Pre-calculated total tokens (burndown already applied if needed).\n        input_tokens: Number of input tokens.\n        output_tokens: Number of output tokens (default 0).\n\n    Returns:\n        AcquireResult with slot time, wait time, queue position, and record ID.\n    \"\"\"\n    # Resolve input tokens and determine if burndown rate should be applied\n    if tokens is not None:\n        if input_tokens is not None:\n            raise ValueError(\"Cannot specify both tokens and input_tokens\")\n        # When tokens= is used, assume burndown is already applied\n        # Use the value directly as effective_combined_tokens\n        input_tokens = tokens\n        effective_combined_tokens = float(tokens)\n    else:\n        if input_tokens is None:\n            raise ValueError(\"Must specify either tokens or input_tokens\")\n        # When input_tokens/output_tokens are used, apply burndown rate\n        effective_combined_tokens = input_tokens + (self._burndown_rate * output_tokens)\n\n    return await self._execute_acquire(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        effective_combined_tokens=effective_combined_tokens,\n    )\n</code></pre>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.adjust","title":"adjust  <code>async</code>","text":"<pre><code>adjust(record_id: str, actual_output: int) -&gt; None\n</code></pre> <p>Adjust the output tokens for a consumption record.</p> <p>Use this when the actual output tokens differ from the estimate. This frees up capacity if actual &lt; estimated, or uses more if actual &gt; estimated.</p> <p>Parameters:</p> Name Type Description Default <code>record_id</code> <code>str</code> <p>The record ID from the acquire() result.</p> required <code>actual_output</code> <code>int</code> <p>The actual number of output tokens.</p> required Source code in <code>src/llmratelimiter/limiter.py</code> <pre><code>async def adjust(self, record_id: str, actual_output: int) -&gt; None:\n    \"\"\"Adjust the output tokens for a consumption record.\n\n    Use this when the actual output tokens differ from the estimate.\n    This frees up capacity if actual &lt; estimated, or uses more if actual &gt; estimated.\n\n    Args:\n        record_id: The record ID from the acquire() result.\n        actual_output: The actual number of output tokens.\n    \"\"\"\n\n    async def do_adjust() -&gt; None:\n        result = await self.redis.eval(  # type: ignore[misc]\n            self._adjust_script,\n            1,\n            self.consumption_key,\n            record_id,\n            actual_output,\n        )\n        if result[0] == 0:\n            logger.warning(\"Record not found for adjustment: %s\", record_id)\n\n    try:\n        if self._retry_config is not None:\n            await retry_with_backoff(do_adjust, self._retry_config, \"adjust\")\n        else:\n            await do_adjust()\n    except RETRYABLE_ERRORS as e:\n        logger.warning(\"Failed to adjust record %s: %s\", record_id, e)\n    except Exception as e:\n        logger.warning(\"Failed to adjust record %s: %s\", record_id, e)\n</code></pre>"},{"location":"api/#llmratelimiter.limiter.RateLimiter.get_status","title":"get_status  <code>async</code>","text":"<pre><code>get_status() -&gt; RateLimitStatus\n</code></pre> <p>Get current rate limit status.</p> <p>Returns:</p> Type Description <code>RateLimitStatus</code> <p>RateLimitStatus with current usage and limits.</p> Source code in <code>src/llmratelimiter/limiter.py</code> <pre><code>async def get_status(self) -&gt; RateLimitStatus:\n    \"\"\"Get current rate limit status.\n\n    Returns:\n        RateLimitStatus with current usage and limits.\n    \"\"\"\n    current_time = time.time()\n\n    async def do_get_status() -&gt; tuple[int, int, int, int]:\n        result = await self.redis.eval(  # type: ignore[misc]\n            self._status_script,\n            1,\n            self.consumption_key,\n            current_time,\n            self.window_seconds,\n        )\n        return (\n            int(result[0]),\n            int(result[1]),\n            int(result[2]),\n            int(result[3]),\n        )\n\n    try:\n        if self._retry_config is not None:\n            total_input, total_output, total_requests, queue_depth = await retry_with_backoff(\n                do_get_status, self._retry_config, \"get_status\"\n            )\n        else:\n            total_input, total_output, total_requests, queue_depth = await do_get_status()\n    except Exception as e:\n        logger.warning(\"Redis error getting status: %s\", e)\n        total_input = 0\n        total_output = 0\n        total_requests = 0\n        queue_depth = 0\n\n    return RateLimitStatus(\n        model=self.model_name,\n        window_seconds=self.window_seconds,\n        tokens_used=total_input + total_output,\n        tokens_limit=self.tpm_limit,\n        input_tokens_used=total_input,\n        input_tokens_limit=self.input_tpm_limit,\n        output_tokens_used=total_output,\n        output_tokens_limit=self.output_tpm_limit,\n        requests_used=total_requests,\n        requests_limit=self.rpm_limit,\n        queue_depth=queue_depth,\n    )\n</code></pre>"},{"location":"api/#models","title":"Models","text":"<p>Data models for results and status.</p> <p>Result dataclasses for rate limiter operations.</p>"},{"location":"api/#llmratelimiter.models.AcquireResult","title":"AcquireResult  <code>dataclass</code>","text":"<p>Result from an acquire() call.</p> <p>Attributes:</p> Name Type Description <code>slot_time</code> <code>float</code> <p>The timestamp when the request is scheduled to execute.</p> <code>wait_time</code> <code>float</code> <p>Time in seconds the caller waited (or will wait).</p> <code>queue_position</code> <code>int</code> <p>Position in the FIFO queue (0 if immediate).</p> <code>record_id</code> <code>str</code> <p>Unique ID for this consumption record (for adjust()).</p> Source code in <code>src/llmratelimiter/models.py</code> <pre><code>@dataclass\nclass AcquireResult:\n    \"\"\"Result from an acquire() call.\n\n    Attributes:\n        slot_time: The timestamp when the request is scheduled to execute.\n        wait_time: Time in seconds the caller waited (or will wait).\n        queue_position: Position in the FIFO queue (0 if immediate).\n        record_id: Unique ID for this consumption record (for adjust()).\n    \"\"\"\n\n    slot_time: float\n    wait_time: float\n    queue_position: int\n    record_id: str\n</code></pre>"},{"location":"api/#llmratelimiter.models.RateLimitStatus","title":"RateLimitStatus  <code>dataclass</code>","text":"<p>Current status of a rate limiter.</p> <p>Unified status for both combined and split mode limiters. Unused fields are set to 0.</p> <p>Combined mode (tpm &gt; 0):     - tokens_used/tokens_limit contain combined token usage     - input_tokens_used/input_tokens_limit are 0     - output_tokens_used/output_tokens_limit are 0</p> <p>Split mode (input_tpm/output_tpm &gt; 0):     - tokens_used/tokens_limit are 0     - input_tokens_used/input_tokens_limit contain input token usage     - output_tokens_used/output_tokens_limit contain output token usage</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model name this limiter is for.</p> <code>window_seconds</code> <code>int</code> <p>The sliding window duration.</p> <code>tokens_used</code> <code>int</code> <p>Current combined tokens consumed (combined mode).</p> <code>tokens_limit</code> <code>int</code> <p>Maximum combined tokens allowed (combined mode).</p> <code>input_tokens_used</code> <code>int</code> <p>Current input tokens consumed (split mode).</p> <code>input_tokens_limit</code> <code>int</code> <p>Maximum input tokens allowed (split mode).</p> <code>output_tokens_used</code> <code>int</code> <p>Current output tokens consumed (split mode).</p> <code>output_tokens_limit</code> <code>int</code> <p>Maximum output tokens allowed (split mode).</p> <code>requests_used</code> <code>int</code> <p>Current requests in the window.</p> <code>requests_limit</code> <code>int</code> <p>Maximum requests allowed per window.</p> <code>queue_depth</code> <code>int</code> <p>Number of pending requests (slot_time &gt; now).</p> Source code in <code>src/llmratelimiter/models.py</code> <pre><code>@dataclass\nclass RateLimitStatus:\n    \"\"\"Current status of a rate limiter.\n\n    Unified status for both combined and split mode limiters.\n    Unused fields are set to 0.\n\n    Combined mode (tpm &gt; 0):\n        - tokens_used/tokens_limit contain combined token usage\n        - input_tokens_used/input_tokens_limit are 0\n        - output_tokens_used/output_tokens_limit are 0\n\n    Split mode (input_tpm/output_tpm &gt; 0):\n        - tokens_used/tokens_limit are 0\n        - input_tokens_used/input_tokens_limit contain input token usage\n        - output_tokens_used/output_tokens_limit contain output token usage\n\n    Attributes:\n        model: The model name this limiter is for.\n        window_seconds: The sliding window duration.\n        tokens_used: Current combined tokens consumed (combined mode).\n        tokens_limit: Maximum combined tokens allowed (combined mode).\n        input_tokens_used: Current input tokens consumed (split mode).\n        input_tokens_limit: Maximum input tokens allowed (split mode).\n        output_tokens_used: Current output tokens consumed (split mode).\n        output_tokens_limit: Maximum output tokens allowed (split mode).\n        requests_used: Current requests in the window.\n        requests_limit: Maximum requests allowed per window.\n        queue_depth: Number of pending requests (slot_time &gt; now).\n    \"\"\"\n\n    model: str\n    window_seconds: int\n    tokens_used: int = 0\n    tokens_limit: int = 0\n    input_tokens_used: int = 0\n    input_tokens_limit: int = 0\n    output_tokens_used: int = 0\n    output_tokens_limit: int = 0\n    requests_used: int = 0\n    requests_limit: int = 0\n    queue_depth: int = 0\n</code></pre>"},{"location":"tests/","title":"Test Documentation","text":"<p>This page documents all 77 tests in the LLMRateLimiter test suite. Tests are organized by category and each test explanation includes what behavior it validates and why it matters.</p>"},{"location":"tests/#test-organization","title":"Test Organization","text":"File Tests Description <code>test_combined_limiter.py</code> 10 Combined TPM+RPM limiting (OpenAI/Anthropic) <code>test_split_limiter.py</code> 11 Split input/output TPM limiting (GCP Vertex AI) <code>test_connection.py</code> 32 Connection management and retry logic <code>test_integration.py</code> 24 End-to-end tests with real Redis"},{"location":"tests/#combined-mode-tests","title":"Combined Mode Tests","text":"<p>Tests for the combined TPM+RPM rate limiting mode used by providers like OpenAI and Anthropic.</p>"},{"location":"tests/#testcombinedmodebasic","title":"TestCombinedModeBasic","text":""},{"location":"tests/#test_acquire_under_limits_returns_immediately","title":"<code>test_acquire_under_limits_returns_immediately</code>","text":"<p>What it tests: When token usage is under configured limits, <code>acquire()</code> returns immediately with zero wait time.</p> <p>Why it matters: This is the happy path - most requests should not be rate limited when capacity is available.</p>"},{"location":"tests/#test_acquire_at_capacity_waits","title":"<code>test_acquire_at_capacity_waits</code>","text":"<p>What it tests: When at capacity, <code>acquire()</code> blocks for the calculated wait time before returning.</p> <p>Why it matters: Validates that the rate limiter actually enforces limits by making callers wait when necessary.</p>"},{"location":"tests/#test_acquire_returns_queue_position","title":"<code>test_acquire_returns_queue_position</code>","text":"<p>What it tests: <code>acquire()</code> returns the correct queue position indicating how many requests are ahead.</p> <p>Why it matters: Queue position helps callers understand their place in line and estimate wait times.</p>"},{"location":"tests/#testcombinedmodeconfig","title":"TestCombinedModeConfig","text":""},{"location":"tests/#test_burst_multiplier_applied","title":"<code>test_burst_multiplier_applied</code>","text":"<p>What it tests: The <code>burst_multiplier</code> configuration correctly increases effective limits (e.g., 1.5x multiplier on 100K TPM = 150K effective limit).</p> <p>Why it matters: Burst multiplier allows temporary spikes above base limits for bursty workloads.</p>"},{"location":"tests/#test_custom_window_seconds","title":"<code>test_custom_window_seconds</code>","text":"<p>What it tests: Custom <code>window_seconds</code> configuration is properly applied to the limiter.</p> <p>Why it matters: Different use cases may need different sliding window durations (e.g., 30s vs 120s).</p>"},{"location":"tests/#test_is_split_mode_false_for_combined","title":"<code>test_is_split_mode_false_for_combined</code>","text":"<p>What it tests: The <code>is_split_mode</code> property returns <code>False</code> for combined-only configurations.</p> <p>Why it matters: Allows code to detect which mode is active and adjust behavior accordingly.</p>"},{"location":"tests/#testcombinedmodegracefuldegradation","title":"TestCombinedModeGracefulDegradation","text":""},{"location":"tests/#test_redis_error_allows_request","title":"<code>test_redis_error_allows_request</code>","text":"<p>What it tests: When Redis fails, <code>acquire()</code> returns immediately with zero wait time instead of blocking.</p> <p>Why it matters: Graceful degradation prevents Redis outages from blocking all LLM API calls.</p>"},{"location":"tests/#testcombinedmodestatus","title":"TestCombinedModeStatus","text":""},{"location":"tests/#test_get_status_returns_correct_info","title":"<code>test_get_status_returns_correct_info</code>","text":"<p>What it tests: <code>get_status()</code> returns accurate usage information including tokens used, requests used, and queue depth.</p> <p>Why it matters: Status monitoring is essential for dashboards and alerting on rate limit usage.</p>"},{"location":"tests/#testcombinedmodeluascript","title":"TestCombinedModeLuaScript","text":""},{"location":"tests/#test_lua_script_receives_correct_arguments","title":"<code>test_lua_script_receives_correct_arguments</code>","text":"<p>What it tests: The Lua script receives correct arguments including key name, token count, and limits.</p> <p>Why it matters: Ensures the Python code correctly marshals data to the Redis Lua script.</p>"},{"location":"tests/#split-mode-tests","title":"Split Mode Tests","text":"<p>Tests for the split input/output TPM rate limiting mode used by providers like GCP Vertex AI.</p>"},{"location":"tests/#testsplitmodebasic","title":"TestSplitModeBasic","text":""},{"location":"tests/#test_acquire_under_limits_returns_immediately_1","title":"<code>test_acquire_under_limits_returns_immediately</code>","text":"<p>What it tests: When both input and output token usage are under limits, <code>acquire()</code> returns immediately.</p> <p>Why it matters: Validates that split mode works correctly when capacity is available.</p>"},{"location":"tests/#test_acquire_with_default_output_tokens","title":"<code>test_acquire_with_default_output_tokens</code>","text":"<p>What it tests: <code>output_tokens</code> parameter defaults to 0 if not specified.</p> <p>Why it matters: Simplifies API for cases where output tokens are unknown or zero.</p>"},{"location":"tests/#testsplitmodeconfig","title":"TestSplitModeConfig","text":""},{"location":"tests/#test_burst_multiplier_applied_to_all_limits","title":"<code>test_burst_multiplier_applied_to_all_limits</code>","text":"<p>What it tests: Burst multiplier is applied to input TPM, output TPM, and RPM limits.</p> <p>Why it matters: Ensures consistent burst behavior across all limit types.</p>"},{"location":"tests/#test_is_split_mode_true_for_split","title":"<code>test_is_split_mode_true_for_split</code>","text":"<p>What it tests: The <code>is_split_mode</code> property returns <code>True</code> for split configurations.</p> <p>Why it matters: Allows code to detect split mode and enable adjust() functionality.</p>"},{"location":"tests/#testsplitmodeadjust","title":"TestSplitModeAdjust","text":""},{"location":"tests/#test_adjust_updates_output_tokens","title":"<code>test_adjust_updates_output_tokens</code>","text":"<p>What it tests: <code>adjust()</code> successfully updates the output tokens for a consumption record.</p> <p>Why it matters: Core functionality for correcting output token estimates after API response.</p>"},{"location":"tests/#test_adjust_handles_not_found","title":"<code>test_adjust_handles_not_found</code>","text":"<p>What it tests: <code>adjust()</code> handles missing records gracefully without raising errors.</p> <p>Why it matters: Records may expire before adjust is called; this shouldn't crash the application.</p>"},{"location":"tests/#test_adjust_works_in_combined_mode","title":"<code>test_adjust_works_in_combined_mode</code>","text":"<p>What it tests: <code>adjust()</code> can be called in combined mode (useful for tracking actual usage).</p> <p>Why it matters: Provides flexibility - adjust() isn't restricted to split mode only.</p>"},{"location":"tests/#testsplitmodestatus","title":"TestSplitModeStatus","text":""},{"location":"tests/#test_get_status_returns_split_info","title":"<code>test_get_status_returns_split_info</code>","text":"<p>What it tests: <code>get_status()</code> returns separate input and output token usage in split mode.</p> <p>Why it matters: Monitoring needs to show both input and output consumption separately.</p>"},{"location":"tests/#testsplitmodegracefuldegradation","title":"TestSplitModeGracefulDegradation","text":""},{"location":"tests/#test_redis_error_allows_request_1","title":"<code>test_redis_error_allows_request</code>","text":"<p>What it tests: Redis errors in split mode also trigger graceful degradation.</p> <p>Why it matters: Ensures fail-open behavior works consistently across all modes.</p>"},{"location":"tests/#testsplitmodeluascript","title":"TestSplitModeLuaScript","text":""},{"location":"tests/#test_lua_script_receives_both_token_types","title":"<code>test_lua_script_receives_both_token_types</code>","text":"<p>What it tests: Lua script receives both input_tokens and output_tokens as separate arguments.</p> <p>Why it matters: Validates correct data marshaling for split mode.</p>"},{"location":"tests/#connection-management-tests","title":"Connection Management Tests","text":"<p>Tests for Redis connection pooling, retry logic, and exponential backoff.</p>"},{"location":"tests/#testretryconfig","title":"TestRetryConfig","text":""},{"location":"tests/#test_default_values","title":"<code>test_default_values</code>","text":"<p>What it tests: RetryConfig has sensible defaults (3 retries, 0.1s base delay, 5.0s max delay).</p> <p>Why it matters: Users should get reasonable behavior without configuration.</p>"},{"location":"tests/#test_custom_values","title":"<code>test_custom_values</code>","text":"<p>What it tests: All RetryConfig parameters can be customized.</p> <p>Why it matters: Different environments need different retry strategies.</p>"},{"location":"tests/#test_zero_retries_allowed","title":"<code>test_zero_retries_allowed</code>","text":"<p>What it tests: <code>max_retries=0</code> is valid (disables retry).</p> <p>Why it matters: Some users may want fail-fast behavior.</p>"},{"location":"tests/#test_negative_retries_rejected","title":"<code>test_negative_retries_rejected</code>","text":"<p>What it tests: Negative <code>max_retries</code> raises ValueError.</p> <p>Why it matters: Prevents configuration errors.</p>"},{"location":"tests/#test_zero_base_delay_rejected","title":"<code>test_zero_base_delay_rejected</code>","text":"<p>What it tests: Zero <code>base_delay</code> raises ValueError.</p> <p>Why it matters: Zero delay would cause tight retry loops.</p>"},{"location":"tests/#test_negative_base_delay_rejected","title":"<code>test_negative_base_delay_rejected</code>","text":"<p>What it tests: Negative <code>base_delay</code> raises ValueError.</p> <p>Why it matters: Negative delays are nonsensical.</p>"},{"location":"tests/#test_max_delay_less_than_base_rejected","title":"<code>test_max_delay_less_than_base_rejected</code>","text":"<p>What it tests: <code>max_delay &lt; base_delay</code> raises ValueError.</p> <p>Why it matters: Max should never be less than base.</p>"},{"location":"tests/#test_exponential_base_less_than_one_rejected","title":"<code>test_exponential_base_less_than_one_rejected</code>","text":"<p>What it tests: <code>exponential_base &lt; 1</code> raises ValueError.</p> <p>Why it matters: Base &lt; 1 would cause delays to decrease over time.</p>"},{"location":"tests/#test_jitter_negative_rejected","title":"<code>test_jitter_negative_rejected</code>","text":"<p>What it tests: Negative jitter raises ValueError.</p> <p>Why it matters: Jitter must be a positive fraction.</p>"},{"location":"tests/#test_jitter_greater_than_one_rejected","title":"<code>test_jitter_greater_than_one_rejected</code>","text":"<p>What it tests: Jitter &gt; 1.0 raises ValueError.</p> <p>Why it matters: Jitter &gt; 100% could cause negative delays.</p>"},{"location":"tests/#test_config_is_frozen","title":"<code>test_config_is_frozen</code>","text":"<p>What it tests: RetryConfig is immutable (frozen dataclass).</p> <p>Why it matters: Prevents accidental modification after creation.</p>"},{"location":"tests/#testcalculatedelay","title":"TestCalculateDelay","text":""},{"location":"tests/#test_first_attempt_uses_base_delay","title":"<code>test_first_attempt_uses_base_delay</code>","text":"<p>What it tests: First retry (attempt 0) uses exactly <code>base_delay</code>.</p> <p>Why it matters: Establishes the baseline for exponential growth.</p>"},{"location":"tests/#test_exponential_growth","title":"<code>test_exponential_growth</code>","text":"<p>What it tests: Delays grow exponentially: 0.1s \u2192 0.2s \u2192 0.4s \u2192 0.8s.</p> <p>Why it matters: Validates the core exponential backoff algorithm.</p>"},{"location":"tests/#test_max_delay_cap","title":"<code>test_max_delay_cap</code>","text":"<p>What it tests: Delays are capped at <code>max_delay</code> regardless of attempt number.</p> <p>Why it matters: Prevents extremely long waits on many retries.</p>"},{"location":"tests/#test_jitter_adds_randomness","title":"<code>test_jitter_adds_randomness</code>","text":"<p>What it tests: Jitter adds \u00b110% variation to delays.</p> <p>Why it matters: Randomization prevents thundering herd on recovery.</p>"},{"location":"tests/#test_delay_never_negative","title":"<code>test_delay_never_negative</code>","text":"<p>What it tests: Delay is always &gt;= 0 even with maximum jitter.</p> <p>Why it matters: Negative sleep times would cause errors.</p>"},{"location":"tests/#testretrywithbackoff","title":"TestRetryWithBackoff","text":""},{"location":"tests/#test_success_on_first_attempt","title":"<code>test_success_on_first_attempt</code>","text":"<p>What it tests: Successful operations return immediately without retry.</p> <p>Why it matters: Retry should only activate on failure.</p>"},{"location":"tests/#test_success_after_retry","title":"<code>test_success_after_retry</code>","text":"<p>What it tests: Operation succeeds after transient failure and retry.</p> <p>Why it matters: Core retry functionality works correctly.</p>"},{"location":"tests/#test_max_retries_exceeded","title":"<code>test_max_retries_exceeded</code>","text":"<p>What it tests: After exhausting retries, the last exception is raised.</p> <p>Why it matters: Retries must eventually give up.</p>"},{"location":"tests/#test_non_retryable_error_raises_immediately","title":"<code>test_non_retryable_error_raises_immediately</code>","text":"<p>What it tests: ResponseError (Lua script error) is not retried.</p> <p>Why it matters: Some errors are permanent and shouldn't be retried.</p>"},{"location":"tests/#test_authentication_error_not_retried","title":"<code>test_authentication_error_not_retried</code>","text":"<p>What it tests: AuthenticationError is not retried.</p> <p>Why it matters: Wrong password won't become right with retries.</p>"},{"location":"tests/#test_timeout_error_is_retried","title":"<code>test_timeout_error_is_retried</code>","text":"<p>What it tests: TimeoutError triggers retry with backoff.</p> <p>Why it matters: Timeouts are transient and often recoverable.</p>"},{"location":"tests/#test_busy_loading_error_is_retried","title":"<code>test_busy_loading_error_is_retried</code>","text":"<p>What it tests: BusyLoadingError (Redis loading data) triggers retry.</p> <p>Why it matters: Redis will become available after loading completes.</p>"},{"location":"tests/#test_zero_retries_no_retry","title":"<code>test_zero_retries_no_retry</code>","text":"<p>What it tests: With <code>max_retries=0</code>, failures raise immediately.</p> <p>Why it matters: Validates fail-fast configuration works.</p>"},{"location":"tests/#test_exponential_delay_timing","title":"<code>test_exponential_delay_timing</code>","text":"<p>What it tests: Actual sleep times match expected exponential delays.</p> <p>Why it matters: Verifies the backoff timing is correct in practice.</p>"},{"location":"tests/#testredisconnectionmanager","title":"TestRedisConnectionManager","text":""},{"location":"tests/#test_default_values_1","title":"<code>test_default_values</code>","text":"<p>What it tests: RedisConnectionManager has sensible defaults (localhost:6379, db=0, 10 connections).</p> <p>Why it matters: Works out of the box for local development.</p>"},{"location":"tests/#test_custom_values_1","title":"<code>test_custom_values</code>","text":"<p>What it tests: All connection parameters can be customized.</p> <p>Why it matters: Production environments need custom configuration.</p>"},{"location":"tests/#test_client_property_creates_pool","title":"<code>test_client_property_creates_pool</code>","text":"<p>What it tests: First access to <code>.client</code> creates the connection pool.</p> <p>Why it matters: Lazy initialization avoids unnecessary connections.</p>"},{"location":"tests/#test_client_property_reuses_pool","title":"<code>test_client_property_reuses_pool</code>","text":"<p>What it tests: Multiple accesses to <code>.client</code> return the same instance.</p> <p>Why it matters: Connection pool should be singleton per manager.</p>"},{"location":"tests/#test_close_cleans_up","title":"<code>test_close_cleans_up</code>","text":"<p>What it tests: <code>close()</code> releases client and pool resources.</p> <p>Why it matters: Proper cleanup prevents resource leaks.</p>"},{"location":"tests/#test_context_manager","title":"<code>test_context_manager</code>","text":"<p>What it tests: <code>async with</code> automatically closes on exit.</p> <p>Why it matters: Pythonic resource management pattern.</p>"},{"location":"tests/#test_close_idempotent","title":"<code>test_close_idempotent</code>","text":"<p>What it tests: Calling <code>close()</code> multiple times is safe.</p> <p>Why it matters: Prevents errors from double-close.</p>"},{"location":"tests/#integration-tests","title":"Integration Tests","text":"<p>End-to-end tests with real Redis that validate the complete system.</p> <p>Redis Required</p> <p>These tests require a running Redis instance. They are automatically skipped if Redis is unavailable.</p>"},{"location":"tests/#testcombinedmodeintegration","title":"TestCombinedModeIntegration","text":""},{"location":"tests/#test_immediate_acquire_under_limits","title":"<code>test_immediate_acquire_under_limits</code>","text":"<p>What it tests: With real Redis, requests under limits complete immediately.</p> <p>Why it matters: Validates the Lua script works correctly with real Redis.</p>"},{"location":"tests/#test_multiple_requests_increment_usage","title":"<code>test_multiple_requests_increment_usage</code>","text":"<p>What it tests: Multiple requests accumulate token and request counts correctly.</p> <p>Why it matters: Verifies consumption tracking works across requests.</p>"},{"location":"tests/#test_rpm_limit_causes_wait","title":"<code>test_rpm_limit_causes_wait</code>","text":"<p>What it tests: Exceeding RPM limit causes wait time on subsequent requests.</p> <p>Why it matters: Validates RPM limiting works end-to-end.</p>"},{"location":"tests/#test_tpm_limit_causes_wait","title":"<code>test_tpm_limit_causes_wait</code>","text":"<p>What it tests: Exceeding TPM limit causes wait time on subsequent requests.</p> <p>Why it matters: Validates TPM limiting works end-to-end.</p>"},{"location":"tests/#test_capacity_freed_after_window","title":"<code>test_capacity_freed_after_window</code>","text":"<p>What it tests: After window expires, consumed capacity is freed.</p> <p>Why it matters: Validates sliding window expiration works correctly.</p>"},{"location":"tests/#test_fifo_ordering","title":"<code>test_fifo_ordering</code>","text":"<p>What it tests: Queued requests get monotonically increasing slot times.</p> <p>Why it matters: Validates FIFO ordering prevents starvation.</p>"},{"location":"tests/#testsplitmodeintegration","title":"TestSplitModeIntegration","text":""},{"location":"tests/#test_immediate_acquire_under_all_limits","title":"<code>test_immediate_acquire_under_all_limits</code>","text":"<p>What it tests: Split mode requests under all limits complete immediately.</p> <p>Why it matters: Validates split mode Lua script with real Redis.</p>"},{"location":"tests/#test_output_tpm_limit_causes_wait","title":"<code>test_output_tpm_limit_causes_wait</code>","text":"<p>What it tests: Exceeding output TPM limit causes wait time.</p> <p>Why it matters: Validates output-specific limiting works.</p>"},{"location":"tests/#test_adjust_updates_record","title":"<code>test_adjust_updates_record</code>","text":"<p>What it tests: <code>adjust()</code> correctly updates output tokens in Redis.</p> <p>Why it matters: Validates the adjust Lua script works correctly.</p>"},{"location":"tests/#test_status_shows_split_info","title":"<code>test_status_shows_split_info</code>","text":"<p>What it tests: Status shows separate input/output token usage from Redis.</p> <p>Why it matters: Validates status Lua script in split mode.</p>"},{"location":"tests/#testmixedmodeintegration","title":"TestMixedModeIntegration","text":""},{"location":"tests/#test_mixed_mode_all_limits_enforced","title":"<code>test_mixed_mode_all_limits_enforced</code>","text":"<p>What it tests: Combined + split limits are all enforced simultaneously.</p> <p>Why it matters: Validates mixed mode works correctly.</p>"},{"location":"tests/#test_mixed_mode_combined_limit_triggers_wait","title":"<code>test_mixed_mode_combined_limit_triggers_wait</code>","text":"<p>What it tests: Combined limit triggers wait even when split limits are OK.</p> <p>Why it matters: All three limits are checked independently.</p>"},{"location":"tests/#test_mixed_mode_input_limit_triggers_wait","title":"<code>test_mixed_mode_input_limit_triggers_wait</code>","text":"<p>What it tests: Input limit triggers wait even when combined limit is OK.</p> <p>Why it matters: Input-specific limiting works in mixed mode.</p>"},{"location":"tests/#test_mixed_mode_output_limit_triggers_wait","title":"<code>test_mixed_mode_output_limit_triggers_wait</code>","text":"<p>What it tests: Output limit triggers wait even when other limits are OK.</p> <p>Why it matters: Output-specific limiting works in mixed mode.</p>"},{"location":"tests/#test_mixed_mode_status_shows_all_info","title":"<code>test_mixed_mode_status_shows_all_info</code>","text":"<p>What it tests: Status shows combined, input, and output usage.</p> <p>Why it matters: Complete visibility in mixed mode.</p>"},{"location":"tests/#testconfigvalidation","title":"TestConfigValidation","text":""},{"location":"tests/#test_mixed_mode_config_is_valid","title":"<code>test_mixed_mode_config_is_valid</code>","text":"<p>What it tests: Configs with both tpm and input_tpm/output_tpm are valid.</p> <p>Why it matters: Mixed mode is explicitly supported.</p>"},{"location":"tests/#test_combined_mode_config","title":"<code>test_combined_mode_config</code>","text":"<p>What it tests: Combined-only config sets correct mode flags.</p> <p>Why it matters: Mode detection works correctly.</p>"},{"location":"tests/#test_split_mode_config","title":"<code>test_split_mode_config</code>","text":"<p>What it tests: Split-only config sets correct mode flags.</p> <p>Why it matters: Mode detection works correctly.</p>"},{"location":"tests/#test_rpm_only_config","title":"<code>test_rpm_only_config</code>","text":"<p>What it tests: RPM-only config (no TPM limits) is valid.</p> <p>Why it matters: Some use cases only need request limiting.</p>"},{"location":"tests/#testdisabledlimits","title":"TestDisabledLimits","text":""},{"location":"tests/#test_disabled_rpm_allows_unlimited_requests","title":"<code>test_disabled_rpm_allows_unlimited_requests</code>","text":"<p>What it tests: With <code>rpm=0</code>, unlimited requests are allowed.</p> <p>Why it matters: Validates limit disabling works correctly.</p>"},{"location":"tests/#testconcurrentrequests","title":"TestConcurrentRequests","text":""},{"location":"tests/#test_concurrent_requests_get_unique_positions","title":"<code>test_concurrent_requests_get_unique_positions</code>","text":"<p>What it tests: Concurrent requests all get unique queue positions and record IDs.</p> <p>Why it matters: Redis Lua scripts handle concurrency correctly.</p>"},{"location":"tests/#testlongwaittimes","title":"TestLongWaitTimes","text":""},{"location":"tests/#test_wait_time_calculation_for_deep_queue","title":"<code>test_wait_time_calculation_for_deep_queue</code>","text":"<p>What it tests: Deep queue correctly calculates long wait times (&gt;4 minutes for 5 requests at full capacity).</p> <p>Why it matters: Wait time calculation is correct even for long queues.</p>"},{"location":"tests/#testconnectionmanagerintegration","title":"TestConnectionManagerIntegration","text":""},{"location":"tests/#test_limiter_with_connection_manager","title":"<code>test_limiter_with_connection_manager</code>","text":"<p>What it tests: RateLimiter works correctly with RedisConnectionManager.</p> <p>Why it matters: Validates the connection manager integration.</p>"},{"location":"tests/#test_connection_manager_ping","title":"<code>test_connection_manager_ping</code>","text":"<p>What it tests: Connection manager can ping Redis successfully.</p> <p>Why it matters: Basic connectivity verification.</p>"},{"location":"tests/#test_multiple_limiters_share_manager","title":"<code>test_multiple_limiters_share_manager</code>","text":"<p>What it tests: Multiple RateLimiter instances can share one connection manager.</p> <p>Why it matters: Efficient connection pooling across limiters.</p>"},{"location":"tests/#test_context_manager_usage","title":"<code>test_context_manager_usage</code>","text":"<p>What it tests: Connection manager works as async context manager in integration.</p> <p>Why it matters: Real-world usage pattern validation.</p>"},{"location":"tests/#running-tests","title":"Running Tests","text":""},{"location":"tests/#run-all-tests","title":"Run All Tests","text":"<pre><code>uv run pytest\n</code></pre>"},{"location":"tests/#run-specific-category","title":"Run Specific Category","text":"<pre><code># Combined mode only\nuv run pytest tests/test_combined_limiter.py\n\n# Integration tests (requires Redis)\nuv run pytest tests/test_integration.py\n</code></pre>"},{"location":"tests/#run-with-coverage","title":"Run with Coverage","text":"<pre><code>uv run pytest --cov --cov-report=html\n</code></pre>"},{"location":"tests/#skip-integration-tests","title":"Skip Integration Tests","text":"<p>If Redis is unavailable, integration tests are automatically skipped.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide covers all the ways to use LLMRateLimiter for different LLM providers and configurations.</p>"},{"location":"usage/#rate-limiting-modes","title":"Rate Limiting Modes","text":"<p>LLMRateLimiter supports three modes based on how your LLM provider counts tokens:</p> Mode Use Case Configuration Combined OpenAI, Anthropic <code>tpm=100000</code> Split GCP Vertex AI <code>input_tpm=4000000, output_tpm=128000</code> Mixed Custom setups Both combined and split limits"},{"location":"usage/#combined-tpm-mode","title":"Combined TPM Mode","text":"<p>Use this mode for providers like OpenAI and Anthropic that have a single tokens-per-minute limit.</p> <pre><code>import asyncio\nfrom llmratelimiter import RateLimiter\n\nasync def main():\n    limiter = RateLimiter(\n        \"redis://localhost:6379\", \"gpt-4\",\n        tpm=100_000,      # 100K tokens per minute\n        rpm=100,          # 100 requests per minute\n    )\n\n    # Recommended: specify input and output tokens separately\n    result = await limiter.acquire(input_tokens=3000, output_tokens=2000)\n    print(f\"Wait time: {result.wait_time:.2f}s, Queue position: {result.queue_position}\")\n\n    # Now safe to make API call\n    # response = await openai.chat.completions.create(...)\n\nasyncio.run(main())\n</code></pre> <p>Recommended: Use input_tokens and output_tokens</p> <p>Always prefer <code>acquire(input_tokens=X, output_tokens=Y)</code> over <code>acquire(tokens=N)</code>. This enables accurate burndown rate calculations for providers like AWS Bedrock, and provides better tracking of token usage.</p>"},{"location":"usage/#acquireresult","title":"AcquireResult","text":"<p>The <code>acquire()</code> method returns an <code>AcquireResult</code> with:</p> <ul> <li><code>slot_time</code>: When the request was scheduled</li> <li><code>wait_time</code>: How long the caller waited (0 if no wait)</li> <li><code>queue_position</code>: Position in queue (0 = immediate)</li> <li><code>record_id</code>: Unique ID for this consumption record</li> </ul>"},{"location":"usage/#split-tpm-mode","title":"Split TPM Mode","text":"<p>Use this mode for providers like GCP Vertex AI that have separate limits for input and output tokens.</p> <pre><code>from llmratelimiter import RateLimiter\n\n# GCP Vertex AI Gemini 1.5 Pro limits\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"gemini-1.5-pro\",\n    input_tpm=4_000_000,   # 4M input tokens per minute\n    output_tpm=128_000,    # 128K output tokens per minute\n    rpm=360,               # 360 requests per minute\n)\n\n# Estimate output tokens upfront\nresult = await limiter.acquire(input_tokens=5000, output_tokens=2048)\n\n# Make API call\nresponse = await vertex_ai.generate(...)\n\n# Adjust with actual output tokens\nawait limiter.adjust(result.record_id, actual_output=response.output_tokens)\n</code></pre>"},{"location":"usage/#why-adjust","title":"Why Adjust?","text":"<p>When using split mode, you must estimate output tokens before the call. After the call completes, use <code>adjust()</code> to correct the estimate:</p> <ul> <li>If actual &lt; estimated: Frees up capacity for other requests</li> <li>If actual &gt; estimated: Uses additional capacity retroactively</li> </ul>"},{"location":"usage/#mixed-mode","title":"Mixed Mode","text":"<p>You can combine both TPM limits for complex scenarios:</p> <pre><code>from llmratelimiter import RateLimiter\n\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"custom-model\",\n    tpm=500_000,           # Combined limit\n    input_tpm=4_000_000,   # Input-specific limit\n    output_tpm=128_000,    # Output-specific limit\n    rpm=360,\n)\n\n# All three limits are checked independently\nresult = await limiter.acquire(input_tokens=5000, output_tokens=2048)\n</code></pre>"},{"location":"usage/#burndown-rate-aws-bedrock","title":"Burndown Rate (AWS Bedrock)","text":"<p>AWS Bedrock uses a \"token burndown rate\" where output tokens count more heavily toward the TPM limit. For example, Claude models on AWS Bedrock use a 5x multiplier for output tokens.</p> <pre><code>from llmratelimiter import RateLimiter\n\n# AWS Bedrock Claude with 5x burndown rate\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"claude-sonnet\",\n    tpm=100_000,         # TPM limit\n    rpm=100,\n    burndown_rate=5.0,   # Output tokens count 5x toward TPM\n)\n\n# Use input_tokens and output_tokens for accurate calculation\nawait limiter.acquire(input_tokens=3000, output_tokens=1000)\n# TPM consumption: 3000 + (5.0 * 1000) = 8000 tokens\n</code></pre> <p>The burndown rate formula is: <code>effective_tpm = input_tokens + (burndown_rate * output_tokens)</code></p> Provider <code>burndown_rate</code> OpenAI/Anthropic (direct) 1.0 (default) AWS Bedrock Claude 5.0 Other providers Check documentation <p>Note</p> <p>The burndown rate only affects the combined TPM limit. Split input/output TPM limits (like GCP Vertex AI) are not affected by the burndown rate.</p>"},{"location":"usage/#connection-management","title":"Connection Management","text":"<p>For production use, use <code>RedisConnectionManager</code> for automatic connection pooling and retry on transient failures.</p>"},{"location":"usage/#basic-connection-manager","title":"Basic Connection Manager","text":"<pre><code>from llmratelimiter import RedisConnectionManager, RateLimiter\n\nmanager = RedisConnectionManager(\n    \"redis://localhost:6379\",\n    max_connections=10,\n)\n\nlimiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)\n</code></pre>"},{"location":"usage/#with-retry-configuration","title":"With Retry Configuration","text":"<pre><code>from llmratelimiter import RedisConnectionManager, RetryConfig\n\nmanager = RedisConnectionManager(\n    \"redis://:secret@redis.example.com:6379\",\n    retry_config=RetryConfig(\n        max_retries=5,        # Retry up to 5 times\n        base_delay=0.1,       # Start with 100ms delay\n        max_delay=10.0,       # Cap at 10 seconds\n        exponential_base=2.0, # Double delay each retry\n        jitter=0.1,           # Add \u00b110% randomness\n    ),\n)\n</code></pre>"},{"location":"usage/#context-manager","title":"Context Manager","text":"<p>Use the connection manager as an async context manager for automatic cleanup:</p> <pre><code>from llmratelimiter import RedisConnectionManager, RateLimiter\n\nasync with RedisConnectionManager(\"redis://localhost:6379\") as manager:\n    limiter = RateLimiter(manager, \"gpt-4\", tpm=100_000, rpm=100)\n    await limiter.acquire(tokens=5000)\n# Connection pool automatically closed\n</code></pre>"},{"location":"usage/#error-handling-and-graceful-degradation","title":"Error Handling and Graceful Degradation","text":"<p>LLMRateLimiter is designed to fail open - if Redis is unavailable, requests are allowed through rather than blocking indefinitely.</p>"},{"location":"usage/#retryable-vs-non-retryable-errors","title":"Retryable vs Non-Retryable Errors","text":"<p>Retryable (automatic retry with backoff): - <code>ConnectionError</code> - Network issues - <code>TimeoutError</code> - Redis timeout - <code>BusyLoadingError</code> - Redis loading data</p> <p>Non-Retryable (fail immediately): - <code>AuthenticationError</code> - Wrong password - <code>ResponseError</code> - Script errors</p>"},{"location":"usage/#example-with-error-handling","title":"Example with Error Handling","text":"<pre><code>from llmratelimiter import RateLimiter, RateLimitConfig\n\nconfig = RateLimitConfig(tpm=100_000, rpm=100)\nlimiter = RateLimiter(manager, \"gpt-4\", config)\n\n# Even if Redis fails, this returns immediately with a valid result\nresult = await limiter.acquire(tokens=5000)\n\nif result.wait_time == 0 and result.queue_position == 0:\n    # Either no rate limiting needed, or Redis was unavailable\n    pass\n\n# Safe to proceed with API call\nresponse = await openai.chat.completions.create(...)\n</code></pre>"},{"location":"usage/#monitoring","title":"Monitoring","text":"<p>Use <code>get_status()</code> to monitor current rate limit usage:</p> <pre><code>status = await limiter.get_status()\n\nprint(f\"Model: {status.model}\")\nprint(f\"Tokens used: {status.tokens_used}/{status.tokens_limit}\")\nprint(f\"Input tokens: {status.input_tokens_used}/{status.input_tokens_limit}\")\nprint(f\"Output tokens: {status.output_tokens_used}/{status.output_tokens_limit}\")\nprint(f\"Requests: {status.requests_used}/{status.requests_limit}\")\nprint(f\"Queue depth: {status.queue_depth}\")\n</code></pre>"},{"location":"usage/#configuration-reference","title":"Configuration Reference","text":""},{"location":"usage/#ratelimiter","title":"RateLimiter","text":"<p>The main class accepts a Redis connection and rate limit configuration:</p> Parameter Type Default Description <code>redis</code> str | Redis | RedisConnectionManager required Redis URL, client, or manager <code>model</code> str required Model name (used for Redis key namespace) <code>config</code> RateLimitConfig None Configuration object (alternative to kwargs) <code>tpm</code> int 0 Combined tokens-per-minute limit <code>rpm</code> int 0 Requests-per-minute limit <code>input_tpm</code> int 0 Input tokens-per-minute limit (split mode) <code>output_tpm</code> int 0 Output tokens-per-minute limit (split mode) <code>window_seconds</code> int 60 Sliding window duration <code>burst_multiplier</code> float 1.0 Multiply limits for burst allowance <code>burndown_rate</code> float 1.0 Output token multiplier for combined TPM (AWS Bedrock: 5.0) <code>password</code> str None Redis password (for URL connections) <code>db</code> int 0 Redis database number (for URL connections) <code>max_connections</code> int 10 Connection pool size (for URL connections) <code>retry_config</code> RetryConfig None Retry configuration (for URL connections)"},{"location":"usage/#ratelimitconfig","title":"RateLimitConfig","text":"Parameter Type Default Description <code>tpm</code> int 0 Combined tokens-per-minute limit <code>input_tpm</code> int 0 Input tokens-per-minute limit <code>output_tpm</code> int 0 Output tokens-per-minute limit <code>rpm</code> int 0 Requests-per-minute limit <code>window_seconds</code> int 60 Sliding window duration <code>burst_multiplier</code> float 1.0 Multiply limits for burst allowance <code>burndown_rate</code> float 1.0 Output token multiplier for combined TPM"},{"location":"usage/#retryconfig","title":"RetryConfig","text":"Parameter Type Default Description <code>max_retries</code> int 3 Maximum retry attempts <code>base_delay</code> float 0.1 Initial delay (seconds) <code>max_delay</code> float 5.0 Maximum delay cap (seconds) <code>exponential_base</code> float 2.0 Backoff multiplier <code>jitter</code> float 0.1 Random variation (0-1)"},{"location":"usage/#redisconnectionmanager","title":"RedisConnectionManager","text":"Parameter Type Default Description <code>url</code> str None Redis URL (e.g., \"redis://localhost:6379\", \"rediss://...\" for SSL) <code>host</code> str \"localhost\" Redis host (if url not provided) <code>port</code> int 6379 Redis port (if url not provided) <code>db</code> int 0 Redis database number <code>password</code> str None Redis password <code>max_connections</code> int 10 Connection pool size <code>retry_config</code> RetryConfig RetryConfig() Retry configuration"},{"location":"usage/#ssl-connections","title":"SSL Connections","text":"<p>Use the <code>rediss://</code> URL scheme for SSL/TLS connections:</p> <pre><code>from llmratelimiter import RateLimiter\n\n# SSL connection\nlimiter = RateLimiter(\"rediss://localhost:6379\", \"gpt-4\", tpm=100_000, rpm=100)\n\n# SSL with password in URL\nlimiter = RateLimiter(\"rediss://:secret@localhost:6379\", \"gpt-4\", tpm=100_000)\n\n# SSL with password as kwarg\nlimiter = RateLimiter(\"rediss://localhost:6379\", \"gpt-4\", tpm=100_000, password=\"secret\")\n</code></pre>"},{"location":"usage/#connection-options","title":"Connection Options","text":"<p>You can pass Redis connection parameters directly to RateLimiter:</p> <pre><code>from llmratelimiter import RateLimiter\n\n# With password\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"gpt-4\",\n    tpm=100_000,\n    password=\"secret\",\n)\n\n# With database number\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"gpt-4\",\n    tpm=100_000,\n    db=2,\n)\n\n# With connection pool size\nlimiter = RateLimiter(\n    \"redis://localhost:6379\", \"gpt-4\",\n    tpm=100_000,\n    max_connections=20,\n)\n\n# All options combined\nlimiter = RateLimiter(\n    \"rediss://localhost:6379\", \"gpt-4\",\n    tpm=100_000,\n    password=\"secret\",\n    db=2,\n    max_connections=20,\n)\n</code></pre> <p>Note</p> <p>When both URL and kwargs specify the same parameter (e.g., password), the kwarg value takes precedence.</p>"},{"location":"usage/#how-it-works","title":"How It Works","text":"<p>This section explains the internal architecture and algorithms used by LLMRateLimiter.</p>"},{"location":"usage/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TB\n    subgraph Client[\"Client Application\"]\n        App[Your LLM App]\n    end\n\n    subgraph RateLimiter[\"LLMRateLimiter\"]\n        RL[RateLimiter]\n        Config[RateLimitConfig]\n        CM[RedisConnectionManager]\n        Retry[RetryConfig]\n    end\n\n    subgraph Redis[\"Redis Server\"]\n        SortedSet[(Sorted Set&lt;br/&gt;consumption records)]\n        Lua[Lua Scripts&lt;br/&gt;Atomic Operations]\n    end\n\n    subgraph LLM[\"LLM Provider\"]\n        API[OpenAI / Anthropic / Vertex AI]\n    end\n\n    App --&gt; |1. acquire&lt;br/&gt;tokens| RL\n    RL --&gt; |2. Execute Lua| Redis\n    Redis --&gt; |3. Wait time| RL\n    RL --&gt; |4. Sleep if needed| RL\n    RL --&gt; |5. Return| App\n    App --&gt; |6. API call| API\n    API --&gt; |7. Response| App\n    App --&gt; |8. adjust&lt;br/&gt;optional| RL\n\n    Config --&gt; RL\n    CM --&gt; RL\n    Retry --&gt; CM</code></pre>"},{"location":"usage/#acquire-flow","title":"Acquire Flow","text":"<p>When you call <code>acquire()</code>, the following happens atomically in Redis:</p> <pre><code>flowchart TD\n    Start([acquire called]) --&gt; Validate{Validate&lt;br/&gt;parameters}\n    Validate --&gt; |Invalid| Error[Raise ValueError]\n    Validate --&gt; |Valid| ExecLua[Execute Lua Script&lt;br/&gt;Atomically in Redis]\n\n    subgraph Lua[\"Lua Script - Atomic Operation\"]\n        Clean[1. Clean expired records&lt;br/&gt;older than window]\n        Clean --&gt; Scan[2. Scan all active records&lt;br/&gt;sum tokens &amp; requests]\n        Scan --&gt; Check{3. Check all limits&lt;br/&gt;TPM, Input, Output, RPM}\n        Check --&gt; |Under limits| FIFO[4. Assign slot after&lt;br/&gt;last request + \u03b5]\n        Check --&gt; |Over any limit| FindSlot[4. Find when enough&lt;br/&gt;capacity frees up]\n        FindSlot --&gt; FIFO2[5. Assign slot at&lt;br/&gt;expiry time + \u03b5]\n        FIFO --&gt; Record[6. Store consumption&lt;br/&gt;record in sorted set]\n        FIFO2 --&gt; Record\n        Record --&gt; Return[7. Return slot_time,&lt;br/&gt;wait_time, queue_pos]\n    end\n\n    ExecLua --&gt; Lua\n    Return --&gt; WaitCheck{wait_time &gt; 0?}\n    WaitCheck --&gt; |Yes| Sleep[asyncio.sleep&lt;br/&gt;wait_time]\n    WaitCheck --&gt; |No| Done\n    Sleep --&gt; Done([Return AcquireResult])</code></pre>"},{"location":"usage/#sliding-window-with-fifo-queue","title":"Sliding Window with FIFO Queue","text":"<p>The rate limiter maintains a sliding window of requests. Past requests count toward the limit, and future requests are queued:</p> <pre><code>flowchart LR\n    subgraph Window[\"60-second Sliding Window\"]\n        direction TB\n\n        subgraph Past[\"Past Records (counting)\"]\n            R1[\"Request 1&lt;br/&gt;1000 tokens&lt;br/&gt;t=0s\"]\n            R2[\"Request 2&lt;br/&gt;2000 tokens&lt;br/&gt;t=5s\"]\n            R3[\"Request 3&lt;br/&gt;1500 tokens&lt;br/&gt;t=15s\"]\n        end\n\n        subgraph Now[\"Current Time t=30s\"]\n            Current((NOW))\n        end\n\n        subgraph Future[\"Future Slots (queued)\"]\n            R4[\"Request 4&lt;br/&gt;3000 tokens&lt;br/&gt;t=35s\"]\n            R5[\"Request 5&lt;br/&gt;2500 tokens&lt;br/&gt;t=40s\"]\n        end\n    end\n\n    subgraph Expired[\"Expired (removed)\"]\n        Old[\"Old requests&lt;br/&gt;before t=-30s\"]\n    end\n\n    Old -.-&gt; |Cleanup| Past\n    Past --&gt; Current\n    Current --&gt; Future</code></pre>"},{"location":"usage/#rate-limit-modes-comparison","title":"Rate Limit Modes Comparison","text":"<pre><code>flowchart TB\n    subgraph Combined[\"Combined Mode (OpenAI/Anthropic)\"]\n        C_Config[\"RateLimitConfig(&lt;br/&gt;tpm=100_000,&lt;br/&gt;rpm=100)\"]\n        C_Acquire[\"acquire(tokens=5000)\"]\n        C_Check[\"Check: tokens \u2264 TPM limit\"]\n    end\n\n    subgraph Split[\"Split Mode (Vertex AI)\"]\n        S_Config[\"RateLimitConfig(&lt;br/&gt;input_tpm=4_000_000,&lt;br/&gt;output_tpm=128_000,&lt;br/&gt;rpm=360)\"]\n        S_Acquire[\"acquire(&lt;br/&gt;input_tokens=5000,&lt;br/&gt;output_tokens=2048)\"]\n        S_Check[\"Check: input \u2264 input_limit&lt;br/&gt;AND output \u2264 output_limit\"]\n        S_Adjust[\"adjust(record_id,&lt;br/&gt;actual_output=1500)\"]\n    end\n\n    subgraph Mixed[\"Mixed Mode\"]\n        M_Config[\"RateLimitConfig(&lt;br/&gt;tpm=500_000,&lt;br/&gt;input_tpm=4_000_000,&lt;br/&gt;output_tpm=128_000)\"]\n        M_Check[\"Check ALL limits&lt;br/&gt;independently\"]\n    end\n\n    C_Config --&gt; C_Acquire --&gt; C_Check\n    S_Config --&gt; S_Acquire --&gt; S_Check --&gt; S_Adjust\n    M_Config --&gt; M_Check</code></pre>"},{"location":"usage/#retry-with-exponential-backoff","title":"Retry with Exponential Backoff","text":"<p>When Redis operations fail, the connection manager retries with exponential backoff:</p> <pre><code>flowchart TD\n    Start([Redis Operation]) --&gt; Try{Try Operation}\n    Try --&gt; |Success| Done([Return Result])\n    Try --&gt; |Retryable Error&lt;br/&gt;Connection/Timeout| Retry{Retries&lt;br/&gt;remaining?}\n    Try --&gt; |Non-retryable&lt;br/&gt;Auth/Script Error| Fail([Raise Immediately])\n\n    Retry --&gt; |Yes| Delay[\"Calculate Delay&lt;br/&gt;base \u00d7 2^attempt&lt;br/&gt;+ jitter\"]\n    Retry --&gt; |No| Degrade([\"Graceful Degradation&lt;br/&gt;Allow Request Through\"])\n\n    Delay --&gt; Sleep[Sleep delay seconds]\n    Sleep --&gt; Try\n\n    subgraph Delays[\"Exponential Backoff Example\"]\n        D0[\"Attempt 0: 0.1s\"]\n        D1[\"Attempt 1: 0.2s\"]\n        D2[\"Attempt 2: 0.4s\"]\n        D3[\"Attempt 3: 0.8s\"]\n        D0 --&gt; D1 --&gt; D2 --&gt; D3\n    end</code></pre>"},{"location":"usage/#capacity-calculation","title":"Capacity Calculation","text":"<p>The Lua script calculates whether capacity is available by checking all configured limits:</p> <pre><code>flowchart TD\n    subgraph Input[\"New Request\"]\n        Req[\"input: 5000 tokens&lt;br/&gt;output: 2000 tokens\"]\n    end\n\n    subgraph Current[\"Current Window State\"]\n        State[\"Combined: 80,000 / 100,000&lt;br/&gt;Input: 50,000 / unlimited&lt;br/&gt;Output: 30,000 / 50,000&lt;br/&gt;Requests: 80 / 100\"]\n    end\n\n    subgraph Calc[\"Capacity Needed\"]\n        Combined[\"Combined needed:&lt;br/&gt;7000 - (100k - 80k) = -13k \u2713\"]\n        Output[\"Output needed:&lt;br/&gt;2000 - (50k - 30k) = -18k \u2713\"]\n        RPM[\"RPM needed:&lt;br/&gt;1 - (100 - 80) = -19 \u2713\"]\n    end\n\n    subgraph Result[\"Result\"]\n        Immediate[\"All negative = Under limits&lt;br/&gt;\u2192 Immediate slot (no wait)\"]\n    end\n\n    Input --&gt; Calc\n    Current --&gt; Calc\n    Calc --&gt; Result</code></pre> <p>If any capacity check is positive (over limit), the script finds the earliest time when enough records expire to free up the required capacity.</p>"}]}